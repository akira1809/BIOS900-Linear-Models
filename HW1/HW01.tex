\documentclass[11pt]{article}\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}

\usepackage{amsfonts}

\usepackage{fancyhdr}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsrefs}
\usepackage{ulem}
%\usepackage[dvips]{graphicx}
\usepackage{bm}
\usepackage{cancel}
\usepackage{color}

\setlength{\headheight}{26pt}
\setlength{\oddsidemargin}{0in}
\setlength{\textwidth}{6.5in}
\setlength{\textheight}{8.5in}

\topmargin 0pt
%Forrest Shortcuts
\newtheorem{defn}{Definition}
\newtheorem{thm}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{pf}{Proof}
\newtheorem{sol}{Solution}
\newcommand{\R}{{\ensuremath{\mathbb R}}}
\newcommand{\J}{{\ensuremath{\mathbb J}}}
\newcommand{\Z}{{\mathbb Z}}
\newcommand{\N}{{\mathbb N}}
\newcommand{\T}{{\mathbb T}}
\newcommand{\Q}{{\mathbb Q}}
\newcommand{\st}{{\text{\ s.t.\ }}}
\newcommand{\rto}{\hookrightarrow}
\newcommand{\rtto}{\hookrightarrow\rightarrow}
\newcommand{\tto}{\to\to}
\newcommand{\C}{{\mathbb C}}
\newcommand{\ep}{\epsilon}
%CJ shortcuts
\newcommand{\thin}{\thinspace}
\newcommand{\beps}{\boldsymbol{\epsilon}}
\newcommand{\bwoc}{by way of contradiction}

%Munkres formatting?
%\renewcommand{\theenumii}{\alph{enumi}}
\renewcommand{\labelenumi}{\theenumi.}
\renewcommand{\theenumii}{\alph{enumii}}
\renewcommand{\labelenumii}{(\theenumii)}

\title{HW1}
\author{Guanlin Zhang}

\lhead{Dr Milind Phadnis
 \\BIOS 900} \chead{}
\rhead{Guanlin Zhang\\Fall '17} \pagestyle{fancyplain}
%\maketitle
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}
Question $\# 1$
\begin{sol}
	For part $[A]$:
	For part $(i)$:\vskip 2mm
	when ${\bf A}$ is an orthogoal matrix, we have by definition:
	\begin{align*}
		&\ {\bf A}^T{\bf A} = {\bf A}{\bf A}^T = I\\
		&\Longrightarrow |{\bf A}{\bf A}^T| = |{\bf A}|\cdot |{\bf A}^T| = |{\bf A}|\cdot |{\bf A}| = |{\bf A}|^2 = 1\\
		&\Longrightarrow |{\bf A}| = 1 \text{ or } -1
	\end{align*}
	For part $(ii)$:\vskip 2mm
	when ${\bf A}$ is idempotent, we have:
	\begin{align*}
		&\ |{\bf A}^2| = |{\bf A}|^2 = |{\bf A}|\\
		&\Longrightarrow |{\bf A}|(|{\bf A}| - 1) = 0\\
		&\Longrightarrow |{\bf A}| = 0 \text{ or }|{\bf A}| = 1
	\end{align*}
	For part $[B]$:\vskip 2mm
	since ${\bf A}$ is symmetric, there is orthogonal matrix ${\bf C}$ and diagonal matrix ${\bf D}$ such that
	\begin{align*}
		{\bf A} ={\bf C} {\bf D}{\bf C}^T
	\end{align*}
	Here
	\begin{align*}
		{\bf D} &= \left[\begin{array}{cc} {\bf I}_r& {\bf 0}\\ {\bf 0}& {\bf 0}\end{array}\right]
	\end{align*}
	with ${\bf I}_r$ as the identity matrix of dimension $r$, and $r$ is the rank of matrix {\bf A}.\vskip 2mm
	Then we can rewrite ${\bf D}$ as
	\begin{align*}
		{\bf D} &= \left[\begin{array}{c}{\bf I}_r\\ {\bf 0}\end{array}\right]\cdot \left[\begin{array}{cc} {\bf I}_r& {\bf 0}\end{array}\right]
	\end{align*}
	So 
	\begin{align*}
		{\bf A} &= {\bf C}\left[\begin{array}{c}{\bf I}_r\\ {\bf 0}\end{array}\right]\cdot \left[\begin{array}{cc} {\bf I}_r& {\bf 0}\end{array}\right]{\bf C}^T
	\end{align*}
	If we define
	\begin{align*}
		{\bf B} &= {\bf C}\left[\begin{array}{c}{\bf I}_r\\ {\bf 0}\end{array}\right]
	\end{align*}
	then 
	\begin{align*}
		{\bf A} &= {\bf B}{\bf B}^T
	\end{align*}
	and
	\begin{align*}
		{\bf B}^T{\bf B} &= \left[\begin{array}{cc} {\bf I}_r& {\bf 0}\end{array}\right]\cdot \left[\begin{array}{c}{\bf I}_r\\ {\bf 0}\end{array}\right] = {\bf I}_r
	\end{align*}
	Thus completed the proof.
\end{sol}

Question $\# 2$:
\begin{sol}
	For part $[A]$:\vskip 2mm
	For textbook problem $2.31$:\vskip 2mm
	we want to show that given:
	\begin{align*}
		{\bf A} = \left[\begin{array}{cc} {\bf A}_{11}& {\bf A}_{12}\\{\bf A}_{21} & {\bf A}_{22}\end{array}\right]
	\end{align*}
	The inverse of ${\bf A}$ is:
	\begin{align*}
		{\bf A}^{-1} &= \left[\begin{array}{cc} {\bf A}_{11}^{-1} + {\bf A}_{11}^{-1}{\bf A}_{12}{\bf B}^{-1}{\bf A}_{21}{\bf A}_{11}^{-1}& -{\bf A}_{11}^{-1}{\bf A}_{12}{\bf B}^{-1}\\ -{\bf B}^{-1}{\bf A}_{21}{\bf A}_{11}^{-1}& {\bf B}^{-1}\end{array}\right]
	\end{align*}
	with 
	\begin{align*}
	{\bf B} &= {\bf A}_{22} - {\bf A}_{21}{\bf A}_{11}^{-1}{\bf A}_{12}
	\end{align*}
	Let's prove this by verifying that the mutiplication of two matrices above is identity matrix.
	\begin{align*}
		&\ \left[\begin{array}{cc} {\bf A}_{11}& {\bf A}_{12}\\{\bf A}_{21} & {\bf A}_{22}\end{array}\right]\cdot  \left[\begin{array}{cc} {\bf A}_{11}^{-1} + {\bf A}_{11}^{-1}{\bf A}_{12}{\bf B}^{-1}{\bf A}_{21}{\bf A}_{11}^{-1}& -{\bf A}_{11}^{-1}{\bf A}_{12}{\bf B}^{-1}\\ -{\bf B}^{-1}{\bf A}_{21}{\bf A}_{11}^{-1}& {\bf B}^{-1}\end{array}\right]\\
		& = \left[\begin{array}{cc}{\bf A}_{11}({\bf A}_{11}^{-1} + {\bf A}_{11}^{-1}{\bf A}_{12}{\bf B}^{-1}{\bf A}_{21}{\bf A}_{11}^{-1}) + {\bf A}_{12}(-{\bf B}^{-1}{\bf A}_{21}{\bf A}_{11}^{-1}), & {\bf A}_{11}(-{\bf A}_{11}^{-1}{\bf A}_{12}{\bf B}^{-1}) + {\bf A}_{12}{\bf B}^{-1}\\ {\bf A}_{21}({\bf A}_{11}^{-1} + {\bf A}_{11}^{-1}{\bf A}_{12}{\bf B}^{-1}{\bf A}_{21}{\bf A}_{11}^{-1}) + {\bf A}_{22}(-{\bf B}^{-1}{\bf A}_{21}{\bf A}_{11}^{-1}),& {\bf A}_{21}(-{\bf A}_{11}^{-1}{\bf A}_{12}{\bf B}^{-1}) + {\bf A}_{22}{\bf B}^{-1} \end{array}\right]
	\end{align*}
	we will call the result of the multiplication matrix above as:
	\begin{align*}
		{\bf C} &= \left[\begin{array}{cc} {\bf C}_{11}& {\bf C}_{12}\\ {\bf C}_{21}& {\bf C}_{22}\end{array}\right]
	\end{align*}
	and we are going to check that ${\bf C} = {\bf I}$.\vskip 2mm
	Denote ${\bf I}_{11}$ as the identity matrix of the same dimension as ${\bf A}_{11}$ and ${\bf I}_{22}$ as the identity matrix of the same dimension as ${\bf A}_{22}$. \vskip 2mm
	We have:
	\begin{align*}
		{\bf C}_{11} &= {\bf A}_{11}({\bf A}_{11}^{-1} + {\bf A}_{11}^{-1}{\bf A}_{12}{\bf B}^{-1}{\bf A}_{21}{\bf A}_{11}^{-1}) + {\bf A}_{12}(-{\bf B}^{-1}{\bf A}_{21}{\bf A}_{11}^{-1}) \\
		&= {\bf I}_{11} + {\bf I}_{11}{\bf A}_{12}{\bf B}^{-1}{\bf A}_{21}{\bf A}_{11}^{-1} - {\bf A}_{12}{\bf B}^{-1}{\bf A}_{21}{\bf A}_{11}^{-1}\\
		&= {\bf I}_{11}
	\end{align*}
	\begin{align*}
		{\bf C}_{12} &={\bf A}_{11}(-{\bf A}_{11}^{-1}{\bf A}_{12}{\bf B}^{-1}) + {\bf A}_{12}{\bf B}^{-1} \\
		&= -{\bf I}_{11}{\bf A}_{12}{\bf B}^{-1} + {\bf A}_{12}{\bf B}^{-1}\\
		&= {\bf 0}
	\end{align*}
	\begin{align*}
		{\bf C}_{21} &= {\bf A}_{21}({\bf A}_{11}^{-1} + {\bf A}_{11}^{-1}{\bf A}_{12}{\bf B}^{-1}{\bf A}_{21}{\bf A}_{11}^{-1}) + {\bf A}_{22}(-{\bf B}^{-1}{\bf A}_{21}{\bf A}_{11}^{-1})\\
		&= {\bf A}_{21}{\bf A}_{11}^{-1} + {\bf A}_{21}{\bf A}_{11}^{-1}{\bf A}_{12}{\bf B}^{-1}{\bf A}_{21}{\bf A}_{11}^{-1} - {\bf A}_{22}{\bf B}^{-1}{\bf A}_{21}{\bf A}_{11}^{-1}\\
		&= {\bf A}_{21}{\bf A}_{11}^{-1} + ({\bf A}_{22} - B){\bf B}^{-1}{\bf A}_{21}{\bf A}_{11}^{-1} - {\bf A}_{22}{\bf B}^{-1}{\bf A}_{21}{\bf A}_{11}^{-1}\\
		&= {\bf A}_{21}{\bf A}_{11}^{-1} + {\bf A}_{22}{\bf B}^{-1}{\bf A}_{21}{\bf A}_{11}^{-1} - {\bf A}_{21}{\bf A}_{11}^{-1} - {\bf A}_{22}{\bf B}^{-1}{\bf A}_{21}{\bf A}_{11}^{-1}\\
		&= 0
	\end{align*}
	For the third $'='$ above, we used the equation that:
	\begin{align*}
		&\ {\bf B} = {\bf A}_{22} - {\bf A}_{21}{\bf A}_{11}^{-1}{\bf A}_{12}\\
		&\Longrightarrow {\bf A}_{21}{\bf A}_{11}^{-1}{\bf A}_{12} = {\bf A}_{22} - {\bf B}
	\end{align*}
	Finally, we have:
	\begin{align*}
		{\bf C}_{22} &= {\bf A}_{21}(-{\bf A}_{11}^{-1}{\bf A}_{12}{\bf B}^{-1}) + {\bf A}_{22}{\bf B}^{-1}\\
		&= -{\bf A}_{21}{\bf A}_{11}^{-1}{\bf A}_{12}{\bf B}^{-1} + {\bf A}_{22}{\bf B}^{-1}\\
		&= -({\bf A}_{22} - {\bf B}){\bf B}^{-1} + {\bf A}_{22}{\bf B}^{-1}\\
		&= -{\bf A}_{22}{\bf B}^{-1} + {\bf I}_{22} + {\bf A}_{22}{\bf B}^{-1}\\
		&= {\bf I}_{22}
	\end{align*}
	Hence we have showed that
	\begin{align*}
		{\bf C} &= \left[\begin{array}{cc} {\bf C}_{11}& {\bf C}_{12}\\ {\bf C}_{21}& {\bf C}_{22}\end{array}\right] = \left[\begin{array}{cc} {\bf I}_{11}& {\bf 0} \\ {\bf 0}& {\bf I}_{22}\end{array}\right]\\
		&= {\bf I}
	\end{align*}
	So the given form is indeed ${\bf A}^{-1}$.\vskip 2mm 
	For problem $2.32$:\vskip 2mm
	We want to show that given the partition matrix:
	\begin{align*}
		{\bf A} &= \left[\begin{array}{cc} {\bf A}_{11}& {\bf a}_{12}\\ {\bf a}'_{12}& a_{22}\end{array}\right]
	\end{align*}
	the inverse is
	\begin{align*}
		{\bf A}^{-1} &= \frac{1}{b}\left[\begin{array}{cc} b{\bf A}_{11}^{-1} + {\bf A}_{11}^{-1}{\bf a}_{12}{\bf a}'_{12}{\bf A}_{11}^{-1}& -{\bf A}_{11}^{-1}{\bf a}_{12}\\ -{\bf a}'_{12}{\bf A}_{11}^{-1}& 1\end{array}\right]
	\end{align*}
	where $b = a_{22} - {\bf a}_{12}'{\bf A}_{11}^{-1}{\bf a}_{12}$.\vskip 2mm
	This is indeed just a special case of problem $2.30$, we could have said that the proof is exactly the same as $2.30$ and skip it. but since it is required as a homework, we present it as following, while using the same notation ${\bf C}$, ${\bf I}_{11}$ and ${\bf I}_{22}$ as in the proof of $2.30$.\vskip 2mm
	We want to verify the multiplication of two given matrices is identity matrix.
	\begin{align*}
		&\ \left[\begin{array}{cc} {\bf A}_{11}& {\bf a}_{12}\\ {\bf a}'_{12}& a_{22}\end{array}\right]\cdot  \frac{1}{b}\left[\begin{array}{cc} b{\bf A}_{11}^{-1} + {\bf A}_{11}^{-1}{\bf a}_{12}{\bf a}'_{12}{\bf A}_{11}^{-1}& -{\bf A}_{11}^{-1}{\bf a}_{12}\\ -{\bf a}'_{12}{\bf A}_{11}^{-1}& 1\end{array}\right]\\
		&= \left[\begin{array}{cc} {\bf A}_{11}({\bf A}_{11}^{-1} + \frac{1}{b}{\bf A}_{11}^{-1}{\bf a}_{12}{\bf a}_{12}'{\bf A}_{11}^{-1}) - \frac{1}{b}{\bf a}_{12}{\bf a}'_{12}{\bf A}_{11}^{-1}, & -\frac{1}{b}{\bf A}_{11}{\bf A}_{11}^{-1}{\bf a}_{12} +\frac{1}{b}{\bf a}_{12}\\ {\bf a}'_{12}({\bf A}_{11}^{-1} + \frac{1}{b}{\bf A}_{11}^{-1}{\bf a}_{12}{\bf a}'_{12}{\bf A}_{11}^{-1}) - \frac{1}{b}a_{22}{\bf a}'_{12}{\bf A}_{11}^{-1},& -\frac{1}{b}{\bf a}'_{12}{\bf A}_{11}^{-1}{\bf a}_{12} + \frac{1}{b}{\bf a}_{22}\end{array}\right]
	\end{align*}
	we call the resulting matrix as ${\bf C}$ :
	\begin{align*}
		{\bf C} &= \left[\begin{array}{cc} {\bf C}_{11}& {\bf C}_{12}\\ {\bf C}_{21}& c_{22}\end{array}\right]
	\end{align*}
	and we show that ${\bf C} = {\bf I}$.\vskip 2mm
	We have:
	\begin{align*}
		{\bf C}_{11} &= {\bf A}_{11}({\bf A}_{11}^{-1} + \frac{1}{b}{\bf A}_{11}^{-1}{\bf a}_{12}{\bf a}_{12}'{\bf A}_{11}^{-1}) - \frac{1}{b}{\bf a}_{12}{\bf a}'_{12}{\bf A}_{11}^{-1}\\
		&= {\bf I}_{11} + \frac{1}{b}{\bf I}_{11}{\bf a}_{12}{\bf a}'_{12}{\bf A}_{11}^{-1} - \frac{1}{b}{\bf a}_{12}{\bf a}_{12}'{\bf A}_{11}^{-1}\\
		&= {\bf I}_{11}
	\end{align*}
	\begin{align*}
		{\bf C}_{12} &= -\frac{1}{b}{\bf A}_{11}{\bf A}_{11}^{-1}{\bf a}_{12} +\frac{1}{b}{\bf a}_{12}\\
		&= -\frac{1}{b}{\bf I}_{11}{\bf a}_{12} + \frac{1}{b}{\bf a}_{12}\\
		&= {\bf 0}
	\end{align*}
	\begin{align*}
		{\bf C}_{21} &= {\bf a}'_{12}({\bf A}_{11}^{-1} + \frac{1}{b}{\bf A}_{11}^{-1}{\bf a}_{12}{\bf a}'_{12}{\bf A}_{11}^{-1}) - \frac{1}{b}a_{22}{\bf a}'_{12}{\bf A}_{11}^{-1}\\
		&= {\bf a}_{12}{\bf A}_{11}^{-1} + \frac{1}{b}{\bf a}_{12}'{\bf A}_{11}^{-1}{\bf a}_{12}{\bf a}_{12}'{\bf A}_{11}^{-1} - \frac{1}{b}{\bf a}_{22}{\bf a}_{12}'{\bf A}_{11}^{-1}\\
		&= {\bf a}'_{12}{\bf A}_{11}^{-1} + \frac{1}{b}(a_{22} - b){\bf a}_{12}'{\bf A}_{11}^{-1} - \frac{1}{b}a_{22}{\bf a}_{12}'{\bf A}_{11}^{-1}\\
		&= {\bf a}_{12}'{\bf A}_{11}^{-1} +\frac{1}{b}a_{22}{\bf a}_{12}'{\bf A}_{11}^{-1} - {\bf a}_{12}'{\bf A}_{11}^{-1} - \frac{1}{b}a_{22}{\bf a}_{12}'{\bf A}_{11}^{-1}\\
		&= {\bf 0}
	\end{align*}
	and finally
	\begin{align*}
		c_{22}  &= -\frac{1}{b}{\bf a}'_{12}{\bf A}_{11}^{-1}{\bf a}_{12} + \frac{1}{b}{\bf a}_{22}\\
		&= -\frac{1}{b}(a_{22} - b) + \frac{1}{b}a_{22}\\
		&= -\frac{1}{b}a_{22} +1+ \frac{1}{b}a_{22}\\
		&= 1
	\end{align*}
	So putting thins together, we got :
	\begin{align*}
		{\bf C} &= \left[\begin{array}{cc} {\bf I}_{11}& {\bf 0}\\ {\bf 0}& 1\end{array}\right] = {\bf I}
	\end{align*}
	which verifies the given form is indeed ${\bf A}^{-1}$.\vskip 2mm
	For problem $2.33$:\vskip 2mm
	assuming all the non-singularity condition holds, we want to check
	\begin{align*}
		({\bf B} + {\bf c}{\bf c}')^{-1} = {\bf B}^{-1} - \frac{{\bf B}^{-1}{\bf c}{\bf c}'{\bf B}^{-1}}{1 + {\bf c}'{\bf B}^{-1}{\bf c}}
	\end{align*}
	We have:
	\begin{align*}
		&\ ({\bf B}+ {\bf c}{\bf c}')\cdot \Big[{\bf B}^{-1} - \frac{{\bf B}^{-1}{\bf c}{\bf c}'{\bf B}^{-1}}{1 + {\bf c}'{\bf B}^{-1}{\bf c}}\Big]\\
		&= {\bf B}{\bf B}^{-1} - {\bf B}\cdot \frac{{\bf B}^{-1}{\bf c}{\bf c}'{\bf B}^{-1}}{1 + {\bf c}'{\bf B}^{-1}{\bf c}} + {\bf c}{\bf c}'{\bf B}^{-1} - {\bf c}{\bf c}'\cdot \frac{{\bf B}^{-1}{\bf c}{\bf c}'{\bf B}^{-1}}{1 + {\bf c}'{\bf B}^{-1}{\bf c}}\\
		&= {\bf I} - \frac{{\bf c}{\bf c}'{\bf B}^{-1}}{1 +{\bf c}'{\bf B}^{-1}{\bf c}} + {\bf c}{\bf c}'{\bf B}^{-1} - \frac{{\bf c}{\bf c}'{\bf B}^{-1}{\bf c}{\bf c}'{\bf B}^{-1}}{1 + {\bf c}'{\bf B}^{-1}{\bf c}}\\
		&= {\bf I} + \frac{-{\bf c}{\bf c}'{\bf B}^{-1} +{\bf c}{\bf c}'{\bf B}^{-1} + {\bf c}{\bf c}'{\bf B}^{-1}{\bf c}'{\bf B}^{-1}{\bf c} - {\bf c}{\bf c}'{\bf B}^{-1}{\bf c}{\bf c}'{\bf B}^{-1} }{1 + {\bf c}'{\bf B}^{-1}{\bf c}}
	\end{align*}
	The first two terms on the numerator are cancelled obviously. For the 3rd and 4th terms, notice that ${\bf c}'{\bf B}^{-1}{\bf c}$ is a quadratic form, and hence a scalar, so we have:
	\begin{align*}
		{\bf c}{\bf c}'{\bf B}^{-1}{\bf c}'{\bf B}^{-1}{\bf c}  &=({\bf c}'{\bf B}^{-1}{\bf c}){\bf c}{\bf c}'{\bf B}^{-1} \\
		{\bf c}{\bf c}'{\bf B}^{-1}{\bf c}{\bf c}'{\bf B}^{-1} &= ({\bf c}'{\bf B}^{-1}{\bf c}){\bf c}{\bf c}'{\bf B}^{-1}
	\end{align*}
	Hence the numerator becomes ${\bf 0}$, and thus we have:
	\begin{align*}
	({\bf B}+ {\bf c}{\bf c}')\cdot \Big[{\bf B}^{-1} - \frac{{\bf B}^{-1}{\bf c}{\bf c}'{\bf B}^{-1}}{1 + {\bf c}'{\bf B}^{-1}{\bf c}}\Big] = {\bf I} + {\bf 0} = {\bf I}
	\end{align*}
	this completes the proof.\vskip 2mm
	For part $[B]$:\vskip 2mm
	Given partition matrix:
	\begin{align*}
		{\bf A} = \left[\begin{array}{cc} {\bf A}_{11}& {\bf A}_{12}\\{\bf A}_{21} & {\bf A}_{22}
		\end{array}\right]
	\end{align*}
	We want to prove that:
	\begin{align*}
		{\bf A}^{-1} &= \left[\begin{array}{cc} {\bf A}_{11}^{-1} + {\bf A}_{11}^{-1}{\bf A}_{12}{\bf B}^{-1}{\bf A}_{21}{\bf A}_{11}^{-1}& -{\bf A}_{11}^{-1}{\bf A}_{12}{\bf B}^{-1}\\ -{\bf B}^{-1}{\bf A}_{21}{\bf A}_{11}^{-1}& {\bf B}^{-1}\end{array}\right]
	\end{align*}
	We derive this proof with Gauss Elimination, and we define $\ast$ to be the left product between matrice, namely:
	\begin{align*}
		{\bf A} \ast {\bf B} = {\bf B} \cdot {\bf A}
	\end{align*}
	So we have:
	\begin{align*}
	&\ 	\left[\begin{array}{cc} {\bf A}_{11}& {\bf A}_{12} \\{\bf A}_{21}  & {\bf A}_{22} \end{array}\Big\vert \begin{array}{cc} {\bf I}_{11}& \\ & {\bf I}_{22} \end{array} \right] \stackrel{\text{row }1 \ast -{\bf A}_{21}{\bf A}_{11}^{-1} + \text{row }2  }{\Longrightarrow}\left[\begin{array}{cc} {\bf A}_{11}& {\bf A}_{12}\\ {\bf 0}& {\bf A}_{22} - {\bf A}_{21}{\bf A}_{11}^{-1}{\bf A}_{12}\end{array}\Big\vert \begin{array}{cc} {\bf I}_{11}& {\bf 0}\\ -{\bf A}_{21}{\bf A}_{11}^{-1}& {\bf I}_{22}\end{array}\right]\\
	&= \left[\begin{array}{cc} {\bf A}_{11}& {\bf A}_{12}\\ {\bf 0}& {\bf B}\end{array}\Big\vert \begin{array}{cc} {\bf I}_{11}& {\bf 0}\\ -{\bf A}_{21}{\bf A}_{11}^{-1}& {\bf I}_{22}\end{array}\right]\\
	&\ \stackrel{\text{row } 2 \ast -{\bf A}_{12}{\bf B}^{-1} + \text{ row }1}{\Longrightarrow}\left[\begin{array}{cc} {\bf A}_{11}& {\bf 0}\\ {\bf 0}& {\bf B}\end{array}\Big\vert \begin{array}{cc} {\bf I}_{11} + {\bf A}_{12}{\bf B}^{-1}{\bf A}_{21}{\bf A}_{11}^{-1}& -{\bf A}_{12}{\bf B}^{-1}\\ -{\bf A}_{21}{\bf A}_{11}^{-1}& {\bf I}_{22}\end{array}\right]\\
	&\ \stackrel{\text{row } 1 \times {\bf A}_{11}^{-1}, \text{row }2 \times {\bf B}^{-1}}{\Longrightarrow}\left[\begin{array}{cc} {\bf I}_{11}& \\ & {\bf I}_{22}\end{array}\Big\vert \begin{array}{cc} {\bf A}_{11} + {\bf A}_{11}{\bf A}_{12}{\bf B}^{-1}{\bf A}_{21}{\bf A}_{11}^{-1}& -{\bf A}_{11}{\bf A}_{12}{\bf B}^{-1}\\ -{\bf B}^{-1}{\bf A}_{21}{\bf A}_{11}^{-1}& {\bf B}^{-1} \end{array}\right]
	\end{align*}
	Thus completed the proof.\vskip 2mm
	Now for the case when:
	\begin{align*}
		{\bf A} &= \left[\begin{array}{cc} {\bf A}_{11}& {\bf a}_{12}\\ {\bf a}'_{12}& a_{22}\end{array}\right]
	\end{align*}
	the proof is the same, just need to replace ${\bf A}_{12}$ by ${\bf a}_{12}$ and ${\bf A}_{21}$ by ${\bf a}_{12}'$ and also we could use the fact that $a_{22}$ is a scalar so the inverse is just the reciprocal.\vskip 2mm
	Finally, for the case when 
	\begin{align*}
		{\bf A} &= \left[\begin{array}{cc} {\bf A}_{11}&{\bf 0} \\ {\bf 0}& {\bf A}_{22}\end{array}\right]
	\end{align*}
	we just need to plug in  ${\bf A}_{12} = {\bf 0}$ and ${\bf A}_{21} = {\bf 0}$ into the general result, and we get our conclusion that
	\begin{align*}
		{\bf A} &= \left[\begin{array}{cc} {\bf A}_{11}&{\bf 0} \\ {\bf 0}& {\bf A}_{22}\end{array}\right]^{-1} =  \left[\begin{array}{cc} {\bf A}_{11}^{-1} + {\bf A}_{11}^{-1}{\bf A}_{12}{\bf B}^{-1}{\bf A}_{21}{\bf A}_{11}^{-1}& -{\bf A}_{11}^{-1}{\bf A}_{12}{\bf B}^{-1}\\ -{\bf B}^{-1}{\bf A}_{21}{\bf A}_{11}^{-1}& {\bf B}^{-1}\end{array}\right]\\
		&=  \left[\begin{array}{cc} {\bf A}_{11}^{-1} & {\bf 0}\\ {\bf 0}& {\bf A}_{22}^{-1}\end{array}\right]
	\end{align*}
\end{sol}

Question $\# 3.$
\begin{sol}
	For part $[A]$:\vskip 2mm
	For part $(i)$:\vskip 2mm
	Notice that ${\bf x}^T{\bf x}$ is a scalar so it can switch order with others when doing matrix multiplication. So we have:
	\begin{align*}
		{\bf H}{\bf x} &= ({\bf I} - {\bf x}({\bf x}^T{\bf x})^{-1}{\bf x}^T)\cdot {\bf x}  = \Big[I - ({\bf x}^T{\bf x})^{-1}{\bf x}{\bf x}^T\Big]\cdot {\bf x}\\
		&= {\bf x} - ({\bf x}^T{\bf x})^{-1}{\bf x}({\bf x}^T{\bf x})\\
		&= x - ({\bf x}^T{\bf x})^{-1}({\bf x}^T{\bf x})\cdot {\bf x}\\
		&= {\bf x} - {\bf x} = {\bf 0}
	\end{align*}
	Hence ${\bf x}$ is an eigen vector of ${\bf H}$ under the eigenvalue $\lambda = 0$
	For part $(ii)$:\vskip 2mm
	Suppose that ${\bf v}$ is orthogonal to ${\bf x}$, this means ${\bf x}^T{\bf v}  = 0$, so we have:
	\begin{align*}
		{\bf H}{\bf v} &= [{\bf I} - {\bf x}({\bf x}^T{\bf x})^{-1}{\bf x}^T]{\bf v}\\
		&= {\bf v} -{\bf x}({\bf x}^T{\bf x})^{-1}\underbrace{({\bf x}^T{\bf v})}_{ = 0}\\
		&=  {\bf v} - {\bf 0} = {\bf v}
	\end{align*}
	So ${\bf v}$ is an eigenvector of ${\bf H}$ under the eigenvalue $\lambda = 1$.\vskip 2mm
	For part $(iii)$:\vskip 2mm
	To show that ${\bf H}$ is idempotent, we compute as follows:
	\begin{align*}
		{\bf H}^2 &= \Big[{\bf I} - {\bf x}({\bf x}^T{\bf x})^{-1}{\bf x}^T\Big]\cdot \Big[{\bf I} - {\bf x}({\bf x}^T{\bf x})^{-1}{\bf x}^T\Big]\\
		&= {\bf I} - {\bf x}({\bf x}^T{\bf x})^{-1}{\bf x}^T - {\bf x}({\bf x}^T{\bf x})^{-1}{\bf x}^T + {\bf x}({\bf x}^T{\bf x})^{-1}{\bf x}^T{\bf x}({\bf x}^T{\bf x})^{-1}{\bf x}^T\\
		&= {\bf I} - 2{\bf x}({\bf x}^T{\bf x})^{-1}{\bf x}^T + {\bf x}({\bf x}^T{\bf x})^{-1}{\bf x}^T\\
		&= {\bf I} - {\bf x}({\bf x}^T{\bf x})^{-1}{\bf x}^T\\
		&= {\bf H}
	\end{align*}
	so ${\bf H}$ is verified to be idempotent.\vskip 2mm
	For part $(B)$:\vskip 2mm
	We check that ${\bf A}$ is idempotent:
	\begin{align*}
		{\bf A}^2 = \left[\begin{array}{cc} \frac{1 - \cos(\theta)}{2}& \frac{\sin(\theta)}{2}\\ \frac{\sin(\theta)}{2}& \frac{1 + \cos(\theta)}{2}\end{array}\right]\cdot \left[\begin{array}{cc} \frac{1 - \cos(\theta)}{2}& \frac{\sin(\theta)}{2}\\ \frac{\sin(\theta)}{2}& \frac{1 + \cos(\theta)}{2}\end{array}\right]
	\end{align*}
	we have:
	\begin{align*}
		{\bf A}^2_{11} &= \frac{1 - \cos(\theta)}{2}\cdot \frac{1 - \cos(\theta)}{2} + \frac{\sin(\theta)}{2}\cdot \frac{\sin(\theta)}{2}\\
		&= \frac{1 - 2\cos\theta + \cos^2(\theta)}{4} + \frac{\sin^2(\theta)}{4}\\
		&= \frac{1 - 2\cos\theta + 1}{4}\\
		&= \frac{2(1 - \cos(\theta))}{4}\\
		&= \frac{1 - \cos(\theta)}{2}\\
		&= {\bf A}_{11}
	\end{align*}	
	\begin{align*}
		{\bf A}^2_{12} &= \frac{1 - \cos(\theta)}{2}\cdot \frac{\sin(\theta)}{2} + \frac{\sin(\theta)}{2}\cdot \frac{1 + \cos(\theta)}{2}\\
		&= \frac{\sin(\theta) - \cos(\theta)\sin(\theta)}{4} + \frac{\sin(\theta) + \sin(\theta)\cos(\theta)}{4}\\
		&= \frac{2\sin(\theta)}{4}=  \frac{\sin(\theta)}{2} = {\bf A}_{12}
	\end{align*}
	Also, since ${\bf A}$ is symmetric, so ${\bf A}^2$ is also symmetric, thus we have:
	\begin{align*}
		{\bf A}^2_{21} = {\bf A}^2_{12} = {\bf A}_{12} = {\bf A}_{21}
	\end{align*}
	Finally, 
	\begin{align*}
		{\bf A}^2_{22} &= \frac{\sin(\theta)}{2}\cdot \frac{\sin(\theta)}{2} + \frac{1 + \cos(\theta)}{2}\cdot \frac{1 + \cos(\theta)}{2}\\
		&= \frac{\sin^2(\theta)}{4} - \frac{1 + 2\cos(\theta) + \cos^2(\theta)}{4}\\
		&= \frac{2 + 2\cos(\theta)}{4}\\
		&= \frac{1 + \cos(\theta)}{2} = {\bf A}_{22}
	\end{align*}
	So together we have verified that ${\bf A}^2 = {\bf A}$ and  is idempotent.
\end{sol}

Question $\# 4.$
\begin{sol}
	For part $[A]$:\vskip 2mm
	We run the following SAS code and observe information from log:
	\begin{center}
		\includegraphics[width = 4cm]{hw1q401.jpg}
	\end{center}
	\begin{center}
		\includegraphics[width = 8cm]{hw1q402.jpg}
	\end{center}
	Since there is $5$ observation with $3$ variables, the size of the matrix is $5$ by $3$, and the size of the transpose is then $3$ by $5$.\vskip 2mm
	For part $[B]$:\vskip 2mm
	We run the following code and output from SAS:
	\begin{center}
		\includegraphics[width = 6cm]{hw1q403.jpg}
		\includegraphics[width = 10cm]{hw1q404.jpg}
	\end{center}
	So we have:
	\begin{align*}
		\bar{X} &= (5, 10, 7.2)^T
	\end{align*}
	For part $[C]$:\vskip 2mm
	We first use proc corr to find the covariance matrix $S$ and correlation matrix $R$:
	\begin{center}
		\includegraphics[width = 4cm]{hw1q405.jpg}
	\end{center}
	\begin{center}
		\includegraphics[width = 6cm]{hw1q406.jpg}
		\hskip 2mm
		\includegraphics[width = 6cm]{hw1q407.jpg}
	\end{center}
	We then use proc iml to check if we have the same answer. So under proc iml, we use the following commands:\vskip 2mm
	First read in the data we created:
	\begin{center}
		\includegraphics[width = 4cm]{hw1q408.jpg}
	\end{center}
	then we run the cov and corr function and print it. we also pasted the code togethere here that is need to find the inverse of $S$ for part $[D]$, so in part $[D]$ we will ony past the output.
	\begin{center}
		\includegraphics[width = 8cm]{hw1q409.jpg}
	\end{center}
	we got the following out put for $S$ and $R$:
	\begin{center}
		\includegraphics[width = 8cm]{hw1q410.jpg}
	\end{center}
	As expected both proc corr and proc iml gave the same result.
	for part $[D]$:\vskip 2mm
	we use the $\text{inv}()$ function under proc iml and print out both the inverse function of covariance matrix and the multiplication between covariance matrix and its inverse:
	\begin{center}
		\includegraphics[width = 6cm]{hw1q411.jpg}
	\end{center}
	As expected that the multiplication between covariance and its inverse is identity matrix, although there is apprently some rounding errors on some of the entries.\vskip 2mm
	For part $[E]$:\vskip 2mm
	So we have:
	\begin{align*}
		\bar{{\bf X}}'{\bf S}^{-1} = (5, 10, 7.2)\cdot \left[\begin{array}{ccc} 1.6262901&0.0532319&-1.428571 \\0.0532319 &0.0912548&0\\-1.428571 &0&1.4285714 \end{array}\right]
	\end{align*}
	to compute by hand, we have:
	\begin{align*}
		\bar{{\bf X}}'{\bf S}^{-1}_1 &= 5\times 1.6262901 + 10 \times 0.0532319 + 7.2 \times -1.428571 = -1.621942\\
		\bar{{\bf X}}'{\bf S}^{-1}_2 &= 5\times 0.0532319 + 10\times 0.0912548 + 7.2 \times 0 = 1.178707\\
		\bar{{\bf X}}'{\bf S}^{-1}_3 &= 5\times -1.428571 + 10\times 0 + 7.2 \times 1.4285714 = 3.142859
	\end{align*}
	So we have:
	\begin{align*}
	\bar{{\bf X}}'{\bf S}^{-1} &= (-1.621942,1.178707,3.142859)
	\end{align*}
	Now to verify with SAS, we compute with the following code and print it:
	\begin{center}
		\includegraphics[width = 4cm]{hw1q412.jpg}
	\end{center}
	and our output is:
	\begin{center}
	\includegraphics[width = 6cm]{hw1q413.jpg}
	\end{center}
	and it matches with our computing results by hand.\vskip 2mm
	For part $[F]$:\vskip 2mm
	To compute by hand, we have:
	\begin{align*}
		\bar{{\bf X}}'{\bf S}^{-1}\bar{{\bf X}} = (-1.621942,1.178707,3.142859)\cdot (5, 10, 7.2)^T = 26.30594
	\end{align*}
	To verify with SAS, under proc iml, we use the command
	\begin{center}
		\includegraphics[width=6cm]{hw1q414.jpg}
	\end{center}
	we print the result and it gave us:
	\begin{center}
		\includegraphics[width=4cm]{hw1q415.jpg}
	\end{center}
	there is some rouding error but we almost got the same results as the one computed by hand.\vskip 2mm
	For part $[G]$:\vskip 2mm
	The determinant of ${\bf S}$ is hand computed as following:
	\begin{align*}
		|S| &=\left|\begin{array}{ccc} 6&-3.5&6\\-3.5 &13&-3.5\\6 &-3.5&6.7\end{array}\right|\\
			&= 6\left|\begin{array}{cc}13 & -3.5\\ -3.5& 6.7\end{array}\right| +3.5\left|\begin{array}{cc}-3.5& -3.5\\ 6& 6.7\end{array}\right| + 6\left|\begin{array}{cc}-3.5& 13\\6 &-3.5 \end{array}\right|\\
			&= 6\times (13\times 6.7 - 3.5\times 3.5) + 3.5\times (-3.5\times 6.7 + 3.5\times 6) + 6\times(3.5\times 3.5 - 6 \times 13)
			&= 46.025
	\end{align*}
	To verify with SAS, we use the follwoing code:
	\begin{center}
		\includegraphics[width = 4cm]{hw1q416.jpg}
	\end{center}
	the output is following:
	\begin{center}
		\includegraphics[width = 4cm]{hw1q417.jpg}
	\end{center}
	and the two answers do match.
	For part $[H]$:\vskip 2mm
	The trace of $S$ is the sum of its diagonal entries, so:
	\begin{align*}
		\text{tr}(S) &= 6 + 13 + 6.7 = 25.7
	\end{align*}
	Verify with following SAS code and output:
	\begin{center}
		\includegraphics[width = 4cm]{hw1q418.jpg}
	\end{center}
	\vskip 2mm
	\begin{center}
		\includegraphics[width = 2cm]{hw1q419.jpg}
	\end{center}
	the answers match.\vskip 2mm
	For part $[I]$:\vskip 2mm
	To find the eigenvalues and eigenvectors of ${\bf S}$,  under proc iml, we run the following code:
	\begin{center}
		\includegraphics[width = 6cm]{hw1q420.jpg}
	\end{center}
	print the output, we have:
	\begin{center}
		\includegraphics[width = 6cm]{hw1q421.jpg}
	\end{center}
	The eigenvalues of ${\bf S}^2$ are just square of the eigenvalues for ${\bf S}$. We got:
	\begin{center}
		\includegraphics[width = 6cm]{hw1q422.jpg}
	\end{center}
	(we checked with both directly looking for eigenvalues of ${\bf S}^2$ and also by squaring the eigenvalues of ${\bf S}$, we got the same results.).
	For part $[J]$:\vskip 2mm
	we find the sum of eigenvalues under proc iml:
	\begin{center}
		\includegraphics[width = 6cm]{hw1q423.jpg}
	\end{center}
	The output is as expected that it is the same as the trace we found before:
	\begin{center}
		\includegraphics[width = 6cm]{hw1q424.jpg}
	\end{center}
	For part $[K]$, we manually multiply all the eigenvalues. (I do not knwo how to do multiplication of all the elements inside one vector with SAS)
	We got:
	\begin{align*}
		17.638718\times 7.7234384 \times 0.3378439 = 46.025
	\end{align*}
	which is the same as the determinat of ${\bf S}$ that we found before.
	\vskip 2mm
	For part $[L]$:\vskip 2mm
	we do the pairwise dot product between eigen vectors under proc iml:
	\begin{center}
		\includegraphics[width = 8cm]{hw1q425.jpg}
	\end{center}
	The output is:
	\begin{center}
		\includegraphics[width = 8cm]{hw1q426.jpg}
	\end{center}
	Not worry too much about rounding error, these eigen vectors are orthogonal pairwise.
\end{sol}

Question $\# 5$:
\begin{sol}
	For part $[A]$:\vskip 2mm
	define ${\bf z} = {\bf y} - {\bf x}$, so ${\bf z}$ would be the vector connecting from the end point of ${\bf x}$ to the end point of ${\bf y}$ and these three vectors form a triangle.\vskip 2mm
	In basic geometry we have this rule:
	\begin{align*}
		L_{{\bf z}}^2= L_{{\bf x}}^2 + L_{{\bf y}}^2 - 2L_{{\bf x}}L_{{\bf y}}\cos\theta
	\end{align*}
	on the other hand, we have:
	\begin{align*}
		L_{{\bf z}}^2 &= {\bf z}^T\cdot {\bf z} = ({\bf y} - {\bf x})^T\cdot({\bf y} - {\bf x})\\
		&= L_{{\bf y}}^2 + L_{{\bf x}}^2 - 2{\bf x}^T{{\bf y}}
	\end{align*}
	put the two above together we can build the following equation:
	\begin{align*}
		&\ L_{{\bf x}}^2 + L_{{\bf y}}^2 - 2L_{{\bf x}}L_{{\bf y}}\cos\theta = L_{{\bf y}}^2 + L_{{\bf x}}^2 - 2{\bf x}^T{{\bf y}}\\
		&\Longrightarrow - 2L_{{\bf x}}L_{{\bf y}}\cos\theta = - 2{\bf x}^T{{\bf y}}\\
		&\Longrightarrow \cos\theta = \frac{{\bf x}^T{{\bf y}}}{L_{{\bf x}}L_{{\bf y}}}
	\end{align*}
	For part $[B]$:\vskip 2mm
	We use the equation from part $[A]$:
	\begin{align*}
		\cos(\theta) &= \frac{{\bf x}^T{\bf y}}{L_{{\bf x}}L_{\bf y}} = \frac{(1, 3, 2)\cdot (-2, 1, -1)^T}{\sqrt{1 + 9 + 4}\cdot \sqrt{4 + 1+ 1}} = \frac{-2 + 3 - 2}{\sqrt{84}}\\
		&= -\frac{1}{2\sqrt{21}}
	\end{align*}
	By using the acos function in R we got the angle as $1.680123$ radian, or we can transfer it to degree unit as $96.26395$ degree.\vskip 2mm
	These two vectors are {\bf NOT} linearly independent, since they are not multiple of each other, or in other words, they do not point in either the same or opposite direction, or in other words, they do not expand into the same linear space, or in other words, their linear combination can not be $0$ without having all the coeffecients being $0$.
\end{sol}

Question $\# 6$.
\begin{sol}
	For part $[A]$:\vskip 2mm
	We want to prove theorem 2.12 (e) that if ${\bf A}$ is any $n \times n$ matrix with eigenvalues $\lambda_1, \lambda_2, \ldots, \lambda_n$, then 
	\begin{align*}
		&(i) \hskip 2cm |{\bf A}| = \prod_{i = 1}^n\lambda_i\\
		&(ii) \hskip 2cm \text{tr}({\bf A}) = \sum_{i = 1}^{n}\lambda_i
	\end{align*}
	Theorem $2.12(d)$ assumed the matrix is symmetric but theorem $2.12(e)$ assumes for general matrix. The spectrum decomposition result actually holds for general matrix than just symmetric, so let's feel free to use the result of $2.12(d)$ without assuming ${\bf A}$ being symmetric.\vskip 2mm
	Then we know that there is orthogonal matrix ${\bf C} = ({\bf x}_1, {\bf x}_2, \ldots, {\bf x}_n)$, each ${\bf x}_i$ being the unit eigen vector corresponding to the eigen value $\lambda_i$, such that
	\begin{align*}
		{\bf A} = {\bf C}{\bf D}{\bf C}'
	\end{align*}
	where ${\bf D} = \text{diag}(\lambda_1, \ldots, \lambda_n)$ is the diagonal matrix whose diagnoal elements are the eigen values of ${\bf A}$ from $\lambda_1$ to $\lambda_n$.\vskip 2mm
	So when we take determinant on ${\bf A}$, we have:
	\begin{align*}
		|{\bf A}| &= |{\bf C}{\bf D}{\bf C}'| = |{\bf C}|\cdot |{\bf D}|\cdot |{\bf C}'|\\
		&= |{\bf C}|^2\cdot \prod_{i = 1}^n\lambda_i\\
		&= \prod_{i = 1}^n\lambda_i
	\end{align*}
	The second to the last $'='$ is due to the fact that for any orthogonal matrix, its determinant is either $1$ or $-1$ and any transpose of a matrix has the same determinant as the original matrix.\vskip 2mm
	Now for the trace, we know that since ${\bf A} = {\bf C}{\bf D}{\bf C}'$, we can denote the columns of orthogonal matrix ${\bf C}$ as ${\bf C}_1, {\bf C}_2, \ldots, {\bf C}_n$, and the entry at $(i ,j)$ as $c_{ij}$. Observe that for $a_{ii}$ (the digonal entry of ${\bf A}$ at $(i, i)$), we have:
	\begin{align*}
		a_{ii} &= \sum_{k, \ell}c_{ik}d_{k\ell}c_{\ell i}'\\
				&= \sum_{k, \ell}c_{ik}c_{i\ell}d_{k\ell} \hskip 1cm (c_{\ell i}' = c_{i \ell})\\
				&= \sum_{k = \ell = 1}^n c_{ik}^2\lambda_k \hskip 1cm ({\bf D} \text{ is diagonal})\\
	\end{align*}
	So the trace of ${\bf A}$ will be:
	\begin{align*}
		\text{tr}({\bf A}) &= \sum a_{ii} = \sum_{i = 1}^n\sum_{k = 1}^n\lambda_k c_{ik}^2\\
		&= \sum_{k = 1}^n\lambda_k\sum_{i = 1}^n c_{ik}^2\\
		&= \sum_{k = 1}^n \lambda_k \hskip 1cm (\sum_{i = 1}^n c_{ik}^2 = 1 \text{ since } {\bf C} \text{ is } orthogonal)
	\end{align*}
	Thus finished the proof of $2.12(e)$.\vskip 2mm
	For part $[B]$:\vskip 2mm
	To computer the eigen values and eigen vectors of ${\bf A}$, we do the following hand calculation:
	\begin{align*}
		&\ |\lambda {\bf I} - {\bf A}| = 0 \Longrightarrow \left|\begin{array}{ccc} \lambda - 13&4&-2 \\ 4&\lambda-13&2\\-2&2&\lambda - 10  \end{array}\right| = 0\\
		&\Longrightarrow (\lambda - 13)\left|\begin{array}{cc} \lambda - 13& 2\\ 2&\lambda - 10 \end{array}\right| - 4\left|\begin{array}{cc} 4& 2\\ -2& \lambda - 10\end{array}\right| - 2\left|\begin{array}{cc} 4& \lambda - 13\\ -2& 2\end{array}\right| = 0\\
		&\Longrightarrow (\lambda - 13)\Big[(\lambda - 13)(\lambda - 10) - 4\Big] - 4\Big[4(\lambda - 10) + 4\Big] - 2\Big[8 +2(\lambda - 13)\Big] = 0\\
		&\Longrightarrow (\lambda - 13)(\lambda^2 - 23\lambda + 126) - 4(4\lambda - 36) - 2(2\lambda - 18) = 0\\
		&\Longrightarrow \lambda^3 -36\lambda^2 + 405\lambda - 1458 = 0\\
		&\Longrightarrow (\lambda - 18)(\lambda - 9)^2 = 0
	\end{align*}
	So we have eigenvalues $\lambda_1 = \lambda_2 = 9$ and $\lambda_3 = 18$.\vskip 2mm
	To find the unit eigen vectors, we solve the homogeneous linear equation, by performing Gauss elimination\vskip 2mm
	When $\lambda_1 = \lambda_2 = 9$:
	\begin{align*}
		&\ \lambda{\bf I} - {\bf A} = 9{\bf I} - A = \left[\begin{array}{ccc} -4& 4&-2\\ 4&-4&2 \\ -2&2&-1 \end{array}\right]\\
		&\stackrel{\text{Gauss elimination}}{\Longrightarrow} \left[\begin{array}{ccc} -2&2&-1 \\0 &0&0\\0&0&0 \end{array}\right]
	\end{align*}
	so we have two free varialbe $x_2$ and $x_3$ and from the first row we get 
	\begin{align*}
		&\ -2x_1 + 2x_2 - x_3 = 0\\
		&\Longrightarrow 2x_1 = 2x_2 - x_3\\
		&\Longrightarrow x_1 = x_2 - \frac{1}{2}x_3
	\end{align*}
	So our solution vector is:
	\begin{align*}
		\left[\begin{array}{c} x_2 - \frac{1}{2}x_3\\ x_2\\ x_3\end{array}\right] &= x_2\left[\begin{array}{c} 1\\ 1\\ 0\end{array}\right] + x_3\left[\begin{array}{c} -\frac{1}{2}\\ 0\\1 \end{array}\right]
	\end{align*}
	So two of the eigen vectors for $\lambda = 9$ will be:
	\begin{align*}
		\left[\begin{array}{c} 1\\ 1\\  0\end{array}\right]\hskip 1cm \text{ and }\hskip 1cm \left[\begin{array}{c} -\frac{1}{2}\\ 0\\ 1\end{array}\right]
	\end{align*}
	Since we aim to find the orthogonal matrix that give ${\bf A}$ decomposition, we want to make the eigenvectors be a unitvector, so the two {\bf unit} eigenvectors for $\lambda =9$ would be:
	\begin{align*}
		{\bf C}_1 = \left[\begin{array}{c} \frac{1}{\sqrt{2}}\\ \frac{1}{\sqrt{2}}\\ 0\end{array}\right] \hskip 1cm \text{ and }\hskip 1cm {\bf C}_2= \left[\begin{array}{c} -\frac{1}{\sqrt{5}}\\ 0\\ \frac{2}{\sqrt{5}}\end{array}\right] 
	\end{align*} 
	Similarly, when $\lambda = 18$, we have:
	\begin{align*}
		\lambda {\bf I} - {\bf A} = 18{\bf I} - {\bf A} = \left[\begin{array}{ccc} 5&4& -2\\ 4&5&2 \\-2 &2&8 \end{array}\right]
	\end{align*}
	Use the same Gauss elimination pocess as above, we got the unit eigen vector as:
	\begin{align*}
		{\bf C}_3 = \left[\begin{array}{c} \frac{2}{3}\\ \frac{-2}{3}\\ \frac{1}{3}\end{array}\right]
	\end{align*}
	It is easy to check that ${\bf C}_1, {\bf C}_2$ and ${\bf C}_3$ are pairwisely orthogonal to each other by taking dot product, and since these are unit eigen vectors, we can define:
	\begin{align*}
		{\bf C} = [{\bf C}_1, {\bf C}_2, {\bf C}_3]
	\end{align*}
	and hence ${\bf C}$ is a $3$ by $3$ orthogonal matrix that gives ${\bf A}$ spectral decomposition. Actually we can easily compute:
	\begin{align*}
		{\bf C}'{\bf A}{\bf C} = \left[\begin{array}{ccc}9&&\\ &9&\\ &&18\end{array}\right]
	\end{align*}
	Thus $2.12(d)$ holds true.
\end{sol}

Question $\# 7$.
\begin{sol}
	For part $[A]$:\vskip 2mm
	Before I get to my solution, I would like to comment that, there might be a possible typo for the last element of row $2$. It would make much more sense to input $5$ instead of $-5$, that way when we do Gauss elimiation, there will be 2 rows cancelled out and the rank is 3, and also the first 3 by 3 block of ${\bf A}$ would also have rank $3$ and it is easy to apply $2.8b$.\vskip 2mm
	Now if we do not change anything, the rank of ${\bf A}$ would be $4$ as we will show below, and also the first $4$ by $4$ submatrix of ${\bf A}$ will not be rank $4$, then we will need to use the $5$ step process as is suggested on the book. \vskip 2mm
	Now let's get to our solution:\vskip 2mm
	First find the rank of ${\bf A}$ by performing Gauss elimination:
	\begin{align*}
		&\  \left[\begin{array}{ccccc} 3&7&-3&6&4\\ 1&4&0&2&-5\\ 2&-1&1&4&-9\\ 0&-5&-3&0&-11\\ 0&-9&1&0&1\end{array}\right]\Longrightarrow \left[\begin{array}{ccccc} 1&4&0&2&-5\\ 3&7&-3&6&4\\ 2&-1&1&4&-9\\ 0&-5&-3&0&-11\\ 0&-9&1&0&1\end{array}\right] \Longrightarrow \left[\begin{array}{ccccc} 1&4&0&2&-5\\ 0&-5&-3&0&19\\ 0&-9&1&0&1\\ 0&-5&-3&0&-11\\ 0&-9&1&0&1\end{array}\right]\\
		&\Longrightarrow \left[\begin{array}{ccccc} 1&4&0&2&-5\\ 0&-5&-3&0&19\\ 0&0&-\frac{22}{5}&0&-\frac{166}{5}\\ 0&0&0&0&-30\\ 0&0&0&0&0\end{array}\right]
	\end{align*}
	The echelon matrix shows that the rank of ${\bf A}$ is $4$.(I checked with SAS and it is correct).\vskip 2mm
	Now let's implement the $5$ step process on page $35$ of the textbook.\vskip 2mm
	Step 1: Find any non-singular $4\times 4$ sub matrix ${\bf C}$.\vskip 2mm
	We found that ${\bf C}$ happees to be the $4$ by $4$ upper right sub matrix of ${\bf A}$:
	\begin{align*}
		{\bf C} = \left[\begin{array}{cccc} 7&-3&6&4 \\ 4&0&2&-5\\ -1&1&4&-9\\ -5&-3&0&-11\end{array}\right]
	\end{align*}
	Step 2: Find ${\bf C}^{-1}$ and $({\bf C}^{-1})'$:
	\begin{align*}
		{\bf C}^{-1} &= \left[\begin{array}{cccc} -0.008333&0.2125&-0.09375&-0.022917 \\ -0.108333&0.0125&0.15625&-0.172917\\ 0.1&-0.175&0.1875&-0.0375\\0.0333333 &-0.1&0&-0.033333\end{array}\right]\\
		({\bf C}^{-1})' &= \left[\begin{array}{cccc} -0.008333&-0.108333&0.1&0.0333333 \\0.2125 &0.0125&-0.175&-0.1\\-0.09375 &0.15625&0.1875&0\\-0.022917 &-0.172917&-0.0375&-0.033333\end{array}\right]
	\end{align*}
	Step $3$. Replace the elements of ${\bf C}$ by the element of $({\bf C}^{-1})'$. So we get:
	\begin{align*}
		\left[\begin{array}{ccccc}3 &-0.008333&-0.108333&0.1&0.0333333\\ 1&0.2125&0.0125&-0.175&-0.1\\ 2&-0.09375&0.15625&0.1875&0\\ 0&-0.022917&-0.172917&-0.0375&-0.033333\\ 0&-9&1&0&1\end{array}\right]
	\end{align*}
	Step $4$, replace all other elements in ${\bf A}$ by zeros, so we get:
	\begin{align*}
		\left[\begin{array}{ccccc}0 &-0.008333&-0.108333&0.1&0.0333333\\ 0&0.2125&0.0125&-0.175&-0.1\\ 0&-0.09375&0.15625&0.1875&0\\ 0&-0.022917&-0.172917&-0.0375&-0.033333\\ 0&0&0&0&0\end{array}\right]
	\end{align*}
	Step $5$, transpose the resulting matrix, so we get:
	\begin{align*}
		{\bf A}^- =\left[\begin{array}{ccccc}0 &0&0&0&0\\-0.008333 &0.2125&-0.09375&-0.022917&0\\-0.108333 &0.0125&0.15625&-0.172917&0\\ 0.1&-0.175&0.1875&-0.0375&0\\ 0.0333333&-0.1&0&-0.033333&0 \end{array}\right]
	\end{align*}
	Now we want to use SAS to verify equation $2.58$, which is:
	\begin{align*}
		{\bf A}{\bf A}^-{\bf A}  = {\bf A}
	\end{align*}
	The following code input manually ${\bf A}$ and ${\bf A}^-$ that we have found above, then we printout the result of ${\bf A}{\bf A}^-{\bf A}$(called checka in the code) to see if it is the same as ${\bf A}$.
	\begin{center}
		\includegraphics[width = 12cm]{hw1q427.jpg}
	\end{center}
	Output is:
	\begin{center}
		\includegraphics[width = 12cm]{hw1q428.jpg}
	\end{center}
	It is clear to see that ignore the rounding error, the resulting matrix is the same as ${\bf A}$, so the stpes worked.\vskip 2mm
	For part $[B]$:\vskip 2mm
	Continue with the input we had from $[A]$, we check in SAS that:
	\begin{enumerate}
		\item [(i)] $\text{rank}({\bf A}^-{\bf A}) = \text{rank}({\bf A}{\bf A}^-) = \text{rank}({\bf A}) = r$ (here $r = 4$).
		\item [(ii)] $({\bf A}^-)'$ is a generalized inverse of ${\bf A}'$; that is, $({\bf A}')^- = ({\bf A}^-)'$
		\item [(iii)]${\bf A} = {\bf A}({\bf A}'{\bf A})^-{\bf A}'{\bf A}$ and ${\bf A}' = {\bf A}'{\bf A}({\bf A}'{\bf A})^-{\bf A}'$
		\item [(iv)]$({\bf A}'{\bf A})^-{\bf A}'$ is a generalized inverse of ${\bf A}$, that is, ${\bf A}^- = ({\bf A}'{\bf A})^-{\bf A}'$
		\item [(v)] ${\bf A}({\bf A}'{\bf A})^-{\bf A}'$ is symmetric, has $\text{rank} = r$m and is invariant to the choice of $({\bf A}'{\bf A})^-$; that is, ${\bf A}({\bf A}'{\bf A})^-{\bf A}'$ remains the same, no matter what value of $({\bf A}'{\bf A})^-$ is used.
	\end{enumerate}
	We check one by one:\vskip 2mm
	For $(i)$:\vskip 2mm
	Since proc iml does not have a default function to find the rank of matrices, we do it by finding the trace of multiplication between a matrix and its generalized inverse. This will adds up all the $1$'s in the diagonal which gives us the rank of the matrix.\vskip 2mm
	Code:
	\begin{center}
		\includegraphics[width = 12cm]{hw1q429.jpg}
	\end{center}
	Output: (print all 3 ranks)
	\begin{center}
		\includegraphics[width = 6cm]{hw1q430.jpg}
	\end{center}
	we see that as expected all three ranks are same and equal to $4$.\vskip 2mm
	For $(ii)$:\vskip 2mm
	Code:
	\begin{center}
		\includegraphics[width = 8cm]{hw1q431.jpg}
	\end{center}
	Output: (print both matrices to see if they are the same)
	\begin{center}
		\includegraphics[width = 12cm]{hw1q432.jpg}
	\end{center}
	So we have checked that the two are indeed the same.
	\vskip 2mm
	For part $(iii)$:\vskip 2mm
	${\bf A}$ was already entered in the preivous question. We compute the other 3 matrices in the code here and print them.\vskip 2mm
	Code:
	\begin{center}
		\includegraphics[width = 5cm]{hw1q433.jpg}
	\end{center}
	Output:
	\begin{center}
		\includegraphics[width = 8cm]{hw1q434.jpg}
	\end{center}
	\begin{center}
		\includegraphics[width = 8cm]{hw1q435.jpg}
	\end{center}
	they match as expected if we disregard the rounding error.\vskip 2mm
	For part $(iv)$:\vskip 2mm
	we compute each side separately and print them to see if thy are the same:\vskip 2mm
	Code:
	\begin{center}
		\includegraphics[width = 8cm]{hw1q436.jpg}
	\end{center}
	Output:
	\begin{center}
		\includegraphics[width = 10cm]{hw1q437.jpg}
	\end{center}
	and as expected they match.\vskip 2mm
	For part $(v)$: \vskip 2mm
	It is hard to verify the invariancy of the matrix. However we check ${\bf A}({\bf A}'{\bf A})^{-}{\bf A}'$ that is symmetric by printing out both itself and its transpose. We also compute its rank to see it is $r$ (in this case it is $4$).\vskip 2mm
	Code:
	\begin{center}
		\includegraphics[width = 12cm]{hw1q438.jpg}
	\end{center}
	Output:
	\begin{center}
		\includegraphics[width = 8cm]{hw1q439.jpg}
	\end{center}
	\begin{center}
		\includegraphics[width = 8cm]{hw1q440.jpg}
	\end{center}
	\begin{center}
		\includegraphics[width = 8cm]{hw1q441.jpg}
	\end{center}
	There is a lot of rounding error in the output, most of the high negative power term should be treated as entry $0$, thus the two matrices is indeed the same, which shows symmetry. and the rank is $4$ as expected.\vskip 2mm
	For part $[C]$:\vskip 2mm
	part $(i)$:\vskip 2mm
	Verify equation $2.90$, which is:
	\begin{align*}
		\text{tr}({\bf A}'{\bf A}) = \text{tr}({\bf A}{\bf A'})
	\end{align*}
	Code:
	\begin{center}
		\includegraphics[width = 8cm]{hw1q442.jpg}
	\end{center}
	Output:
	\begin{center}
		\includegraphics[width = 8cm]{hw1q443.jpg}
	\end{center}
	part $(ii)$:\vskip 2mm
	Verify equation $2.92$, which is:
	\begin{align*}
		\text{tr}({\bf C}'{\bf A}{\bf C}) = \text{tr}({\bf A})
	\end{align*}
	Code:
	\begin{center}
		\includegraphics[width = 10cm]{hw1q444.jpg}
	\end{center}
	Output: we print out the traces for both matrices
	\begin{center}
		\includegraphics[width = 10cm]{hw1q445.jpg}
	\end{center}
	and as expected they are the same and equal to $9$.\vskip 2mm
	For part $(iii)$:\vskip 2mm
	Check equation $2.93$, which is:
	\begin{align*}
		\text{tr}({\bf A}^-{\bf A}) = \text{tr}({\bf A}{\bf }A^-) = r
	\end{align*}
	Code:
	\begin{center}
		\includegraphics[width = 10cm]{hw1q446.jpg}
	\end{center}
	\begin{center}
		\includegraphics[width = 8cm]{hw1q447.jpg}
	\end{center}
	and they are both equal to the rank of ${\bf A}$, which is $4$.\vskip 2mm
	part $(iv)$, check equation $2.108$, which is:
	\begin{align*}
		\text{tr}({\bf A}) = \sum_{i = 1}^{n}\lambda_i
	\end{align*}
	Code:
	\begin{center}
		\includegraphics[width = 8cm]{hw1q448.jpg}
	\end{center}
	\begin{center}
		\includegraphics[width = 8cm]{hw1q449.jpg}
	\end{center}
	and it is verified.\vskip 2mm
	Thus completed the solution of questino 7.
 \end{sol}

Question $\# 8.$
\begin{sol}
	For part $[A]$:
	\begin{align*}
		E[{\bf X}_1] &= E\left[\begin{array}{c} x_1\\ x_2\end{array}\right] = \left[\begin{array}{c} \mu_1\\ \mu_2\end{array}\right] = \left[\begin{array}{c} 2\\ 4\end{array}\right]
	\end{align*}
	For part $[B]$:
	\begin{align*}
		{\bf A}{\bf X}_1&= \left[\begin{array}{cc} 1&-1 \\ 1& 1\end{array}\right]\cdot \left[\begin{array}{c} x_1\\ x_2\end{array}\right] = \left[\begin{array}{c} x_1 - x_2\\ x_1 + x_2\end{array}\right]
	\end{align*}
	So
	\begin{align*}
		E[{\bf A}{\bf X}_1] =  \left[\begin{array}{c} \mu_1 - \mu_2\\ \mu_1 + \mu_2\end{array}\right] =   \left[\begin{array}{c} 2 - 4\\ 2 + 4\end{array}\right] =   \left[\begin{array}{c} -2\\ 6\end{array}\right]
	\end{align*}
	For part $[C]$:
	\begin{align*}
		\text{Cov}[{\bf X}_1] &=   \left[\begin{array}{cc}4 & -1\\ -1&3\end{array}\right]
	\end{align*}
	For part $[D]$:
	\begin{align*}
		\text{Cov}[{\bf A}{\bf X}_1] = \text{Cov}\left[\begin{array}{c} x_1 - x_2\\ x_1 + x_2\end{array}\right] = \left[\begin{array}{cc} \text{var}(x_1 - x_2)& \text{cov}(x_1 - x_2, x_1 + x_2)\\ \text{cov}(x_1 - x_2, x_1 + x_2)& \text{var}(x_1 + x_2)\end{array}\right]
	\end{align*}
	We have:
	\begin{align*}
		\text{var}(x_1 - x_2) &= \text{var}(x_1) + \text{var}(x_2) - 2\text{cov}(x_1, x_2)\\
		&= 4 + 3 - 2\times(-1) = 9
	\end{align*}
	\begin{align*}
		\text{cov}(x_1 - x_2, x_1 + x_2) &= \text{var}(x_1) - \text{var}(x_2) \\
		&= 4 - 3 = 1
	\end{align*}
	\begin{align*}
		\text{var}(x_1 + x_2) &= \text{var}(x_1) + \text{var}(x_2) + 2\text{cov}(x_1, x_2)\\
		&= 4 + 3 - 2 = 5
	\end{align*}
	So we have:
	\begin{align*}
		\text{Cov}[{\bf A}{\bf X}_1] = \left[\begin{array}{cc} 9& 1\\ 1&5\end{array}\right]
	\end{align*}
	For part $[E]$:
	\begin{align*}
		E[{\bf X}_2] = \left[\begin{array}{c} \mu_3\\ \mu_4\\ \mu_5\end{array}\right] = \left[\begin{array}{c} -1\\ 3\\ 0\end{array}\right] 
	\end{align*}
	For part $[F]$':
	\begin{align*}
		{\bf B}{\bf X}_2 &= \left[\begin{array}{ccc} 1&1&1\\ 1&1&-2\end{array}\right]\cdot  \left[\begin{array}{c} x_3\\ x_4\\ x_5\end{array}\right] = \left[\begin{array}{c} x_3+ x_4 + x_5\\x_3 + x_4 - 2x_5 \end{array}\right]
	\end{align*}
	So 
	\begin{align*}
		E[{\bf B}{\bf X}_2]&= \left[\begin{array}{c} \mu_3+  \mu_4 +  \mu_5\\ \mu_3 +  \mu_4 - 2 \mu_5 \end{array}\right] =  \left[\begin{array}{c} -1 + 3 + 0\\ -1 + 3 - 2 \times 0 \end{array}\right]  =  \left[\begin{array}{c} 2\\ 2 \end{array}\right] 
	\end{align*}
	For part $[G]$:
	\begin{align*}
		\text{cov}({\bf X}_2) &= \left[\begin{array}{ccc} 6&1&-1\\ 1&4&0\\ -1&0&2\end{array}\right]
	\end{align*}
	For part $[H]$:
	\begin{align*}
		\text{cov}({\bf B}{\bf X}_2) &= \text{cov}\left[\begin{array}{c} x_3+ x_4 + x_5\\x_3 + x_4 - 2x_5 \end{array}\right]\\
		&= \left[\begin{array}{cc} \text{var}(x_3 + x_4 + x_5)&\text{cov}(x_3 + x_4 + x_5, x_3 + x_4 - 2x_5) \\ \text{cov}(x_3 + x_4 + x_5, x_3 + x_4 - 2x_5)& \text{var}(x_3 + x_4 - 2x_5)\end{array}\right]
	\end{align*}
	We have:
	\begin{align*}
		\text{var}(x_3 + x_4 + x_5) &= \text{var}(x_3) +\text{var}(x_4) + \text{var}(x_5) + 2\text{cov}(x_3, x_4) + 2\text{cov}(x_3, x_5) + 2\text{cov}(x_4, x_5)\\
		&= 6 + 4 + 2 + 2\times 1 + 2\times (-1) + 2\times 0\\
		&= 12
	\end{align*}
	\begin{align*}
		&\ \text{cov}(x_3 + x_4 + x_5, x_3 + x_4 - 2x_5) \\
		&=\text{var}(x_3)+\text{var}(x_4)- 2\text{var}(x_5) + 2\text{cov}(x_3,x_4) - \text{cov}(x_3, x_5) - \text{cov}(x_4, x_5)\\
		&= 6 + 4 - 2\times 2 + 2\times 1 - (-1) - 0\\
		&= 9
	\end{align*}
	\begin{align*}
		&\ \text{var}(x_3 + x_4 - 2x_5) \\
		&= \text{var}(x_3) + \text{var}(x_4) + 4\text{var}(x_5) + 2\text{cov}(x_3,x_4) - 4\text{cov}(x_3,x_5) -4\text{cov}(x_4,x_5)\\
		&= 6 + 4 + 4\times2 + 2\times1 - 4\times(-1) - 4 \times 0\\
		&= 24
	\end{align*}
	So we have:
	\begin{align*}
	\text{cov}({\bf B}{\bf X}_2) &=\left[\begin{array}{cc} 12& 9\\9 &24\end{array} \right]
	\end{align*}
	For part $[I]$:
	\begin{align*}
		\text{cov}({\bf X}_1, {\bf X}_2) &= \text{cov}(\left[\begin{array}{c}x_1\\ x_2\end{array}\right],\left[\begin{array}{c}x_3\\ x_4\\ x_5\end{array}\right]) \\
		&= \left[\begin{array}{ccc} \text{cov}(x_1, x_3)&\text{cov}(x_1, x_4)&\text{cov}(x_1, x_5)\\ \text{cov}(x_2, x_3)&\text{cov}(x_2, x_4)& \text{cov}(x_2, x_5)\end{array}\right]\\
		&= \left[\begin{array}{ccc} 0.5&-0.5&0\\ 1&-1& 0\end{array}\right]
	\end{align*}
	For part $[J]$:
	\begin{align*}
		\text{cov}({\bf A}{\bf X}_1, {\bf B}{\bf X}_2) &= \text{cov}(\left[\begin{array}{c}x_1 - x_2\\x_1 + x_2 \end{array}\right], \left[\begin{array}{c}x_3 + x_4 + x_5\\ x_3 + x_4 - 2x_5\end{array}\right])\\
		&= \left[\begin{array}{cc} \text{cov}(x_1 - x_2, x_3 + x_4 + x_5)&\text{cov}(x_1 - x_2, x_3 + x_4 - 2x_5) \\ \text{cov}(x_1 + x_2, x_3+ x_4+ x_5)&\text{cov}(x_1 + x_2, x_3 + x_4 - 2x_5) \end{array}\right]
	\end{align*}
	We have:
	\begin{align*}
		&\ \text{cov}(x_1 - x_2, x_3 + x_4 + x_5) \\
		&= \text{cov}(x_1 ,x_3) +  \text{cov}(x_ 1,x_4) +  \text{cov}(x_ 1,x_5) -  \text{cov}(x_2 ,x_3) -  \text{cov}(x_2 ,x_4) -  \text{cov}(x_2,x_5)\\
		&= 0.5 - 0.5 + 0 - 1 - (-1) - 0\\
		&= 0
	\end{align*}
	\begin{align*}
		&\ \text{cov}(x_1 - x_2, x_3 + x_4 - 2x_5) \\
		&= \text{cov}(x_1 ,x_3 ) +  \text{cov}(x_1,x_4 ) - 2 \text{cov}(x_1 ,x_5 ) -  \text{cov}(x_2 ,x_3 )-  \text{cov}(x_2 ,x_4 ) +2 \text{cov}(x_2,x_5 )\\
		&= 0.5 - 0.5 - 2\times 0 - 1 - (-1) + 2\times 0\\
		&= 0
	\end{align*}
	\begin{align*}
		& \text{cov}(x_1 + x_2, x_3 + x_4+ x_5)\\
		&= \text{cov}(x_1 , x_3 )+\text{cov}(x_1 , x_4 ) + \text{cov}(x_1 , x_5 ) + \text{cov}(x_2 , x_3 )+  \text{cov}(x_2 , x_4 )+ \text{cov}(x_2 , x_5 )\\
		&= 0.5 - 0.5 + 0 + 1 - 1 + 0\\
		&= 0
	\end{align*}
	\begin{align*}	
	&\ \text{cov}(x_1 + x_2, x_3 + x_4 - 2x_5)\\
	&= \text{cov}(x_1 , x_3) + \text{cov}(x_1 , x_4) - 2\text{cov}(x_1 , x_5) + \text{cov}(x_2 , x_3) + \text{cov}(x_2 , x_4) - 2\text{cov}(x_2 , x_5)\\
	&= 0.5-0.5 - 0 + 1 - 1 - 0\\
	&= 0
	\end{align*}
	So we have:
	\begin{align*}
		\text{cov}({\bf A}{\bf X}_1, {\bf B}{\bf X}_2)  &= \left[\begin{array}{cc} 0& 0\\ 0&0 \end{array}\right]
	\end{align*}
	For part $[K]$:
	\begin{align*}
		{\bf P}_{\rho} &= \left[\begin{array}{ccccc} 1&\frac{-1}{\sqrt{4}\sqrt{3}}&\frac{0.5}{\sqrt{4}{\sqrt{6}}}&-\frac{0.5}{\sqrt{4}\sqrt{4}}&0\\ \frac{-1}{\sqrt{4}\sqrt{3}}&1&\frac{1}{\sqrt{3}\sqrt{6}}&\frac{-1}{\sqrt{3}\sqrt{4}}&0\\ \frac{0.5}{\sqrt{6}\sqrt{4}}&\frac{1}{\sqrt{3}\sqrt{6}}&1&\frac{1}{\sqrt{6}\sqrt{4}}&\frac{-1}{\sqrt{6}\sqrt{2}}\\ \frac{-0.5}{\sqrt{4}\sqrt{4}} &\frac{-1}{\sqrt{3}\sqrt{4}}&\frac{1}{\sqrt{6}\sqrt{4}}&1&0\\ 0&0&\frac{-1}{\sqrt{6}\sqrt{2}}&0&1\end{array}\right] = \left[\begin{array}{ccccc}1&-\frac{1}{2\sqrt{3}}&\frac{1}{4\sqrt{6}}&-\frac{1}{8}&0\\ -\frac{1}{2\sqrt{3}}&1&\frac{1}{3\sqrt{2}}&-\frac{1}{2\sqrt{3}}&0\\ \frac{1}{4\sqrt{6}}&\frac{1}{3\sqrt{2}}&1&\frac{1}{2\sqrt{6}}&\frac{-1}{2\sqrt{3}}\\ -\frac{1}{8}&\frac{-1}{2\sqrt{3}}&\frac{1}{2\sqrt{6}}&1&0\\ 0&0&\frac{-1}{2\sqrt{3}}&0&1\end{array}\right]
	\end{align*}
	For part $[L]$:\vskip 2mm
	$\text{rank}(\bf B) = 2$ since the first and second rows are not linearly independent.\vskip 2mm
	This comlete the solutin of quesiton $\#8$.
\end{sol}












\end{document}
