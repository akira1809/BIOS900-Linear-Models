\documentclass[11pt]{article}

\usepackage{amsfonts}

\usepackage{fancyhdr}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsrefs}
\usepackage{ulem}
\usepackage[dvips]{graphicx}
\usepackage{bm}
\usepackage{cancel}
\usepackage{color}

\setlength{\headheight}{26pt}
\setlength{\oddsidemargin}{0in}
\setlength{\textwidth}{6.5in}
\setlength{\textheight}{8.5in}

\topmargin 0pt
%Forrest Shortcuts
\newtheorem{defn}{Definition}
\newtheorem{thm}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{pf}{Proof}
\newtheorem{sol}{Solution}
\newcommand{\R}{{\ensuremath{\mathbb R}}}
\newcommand{\J}{{\ensuremath{\mathbb J}}}
\newcommand{\Z}{{\mathbb Z}}
\newcommand{\N}{{\mathbb N}}
\newcommand{\T}{{\mathbb T}}
\newcommand{\Q}{{\mathbb Q}}
\newcommand{\st}{{\text{\ s.t.\ }}}
\newcommand{\rto}{\hookrightarrow}
\newcommand{\rtto}{\hookrightarrow\rightarrow}
\newcommand{\tto}{\to\to}
\newcommand{\C}{{\mathbb C}}
\newcommand{\ep}{\epsilon}
%CJ shortcuts
\newcommand{\thin}{\thinspace}
\newcommand{\beps}{\boldsymbol{\epsilon}}
\newcommand{\bwoc}{by way of contradiction}

%Munkres formatting?
%\renewcommand{\theenumii}{\alph{enumi}}
\renewcommand{\labelenumi}{\theenumi.}
\renewcommand{\theenumii}{\alph{enumii}}
\renewcommand{\labelenumii}{(\theenumii)}

\title{HW2}
\author{Guanlin Zhang}

\lhead{Dr Milind Phadnis
 \\BIOS 900} \chead{}
\rhead{Guanlin Zhang\\Fall '17} \pagestyle{fancyplain}
%\maketitle

\begin{document}
Question $\# 1$.
\begin{sol}
	For part $[A]$:\vskip 2mm
	For $(i)$:\vskip 2mm
	We want to prove equation:
	\begin{align*}
		|{\bf S}| = (s_{11}s_{22}\cdots s_{pp})|{\bf R}|
	\end{align*}
	where
	\begin{align*}
		{\bf S} &= \left[\begin{array}{cccc} s_{11}&s_{12}&\cdots&s_{1p}\\ s_{12}&s_{22}&\cdots&s_{2p}\\ \vdots&\vdots&\ddots&\vdots\\ s_{1p}&s_{2p}&\cdots&s_{pp}\end{array}\right]
	\end{align*}
	with
	\begin{align*}
		s_{ik} &= \frac{1}{n}\sum_{j = 1}^{n}(x_{ji} - \bar{x}_i)(x_{jk} - \bar{x}_k)
	\end{align*}
	and
	\begin{align*}
		{\bf R} &= \left[\begin{array}{cccc} 1&r_{12}&\cdots&r_{1p}\\ r_{12}&1&\cdots&r_{2p}\\ \vdots&\vdots&\ddots&\vdots\\ r_{1p}&r_{2p}&\cdots&1\end{array}\right] = \left[\begin{array}{cccc} 1&\frac{s_{12}}{\sqrt{s_{11}s_{22}}}&\cdots&\frac{s_{1p}}{\sqrt{s_{11}s_{pp}}}\\ \frac{s_{12}}{\sqrt{s_{11}s_{22}}}&1&\cdots&\frac{s_{2p}}{\sqrt{s_{22}s_{pp}}}\\ \vdots&\vdots&\ddots&\vdots\\ \frac{s_{1p}}{\sqrt{s_{11}s_{pp}}}&\frac{s_{2p}}{\sqrt{s_{22}s_{pp}}}&\cdots&1\end{array}\right]
	\end{align*}
	with
	\begin{align*}
		r_{ik} &= \frac{s_{ik}}{\sqrt{s_{ii}s_{kk}}} \hskip 1cm r_{ii} = \frac{s_{ii}}{\sqrt{s_{ii}s_{ii}}} = 1
	\end{align*}
	We prove this by using the Lebniz formula (also called Laplace formula) for determinant.\vskip 2mm
	First of all, we denote by $S_p$ the set of permuations of integer set $\{1, 2, \ldots, p\}$. (we did not use bold font for $S_p$, so it should not be confused with the sample variance matrix ${\bf S}$).\vskip 2mm
	So an element $\sigma \in S_p$ represents a particular permutation for $\{1, 2, \ldots, p\}$. \vskip 2mm
	for example, if $\sigma$ make the following permutation:
	\begin{align*}
		\{1, 2, \ldots, p\}\stackrel{\sigma}{\to} \{2, 1, 3, 4, \ldots, p\}
	\end{align*} 
	then we denote $\sigma_1 = 2, \sigma_2 = 1, \sigma_3 = 3, \sigma_4 = 4, \ldots, \sigma_p = p$.\vskip 2mm
	Also, we use $\text{sgn}(\sigma)$ to denote the signature of $\sigma$, a value that is $+1$ whenever the permutation given by $\sigma$ can be achieved by successively interchanging two entries an even number of times, and $-1$ whenever it can be achieved by an odd number of such interchanges.\vskip 2mm
	Then the Leibniz formula (or Laplace formula) says:
	\begin{align*}
		|{\bf S}| &= \Sigma_{\sigma \in S_p}\Big(\text{sgn}(\sigma)\prod_{i = 1}^p s_{i, \sigma_i}\Big)
	\end{align*}
	and 
	\begin{align*}
		|{\bf R}| &= \Sigma_{\sigma \in S_p}\Big(\text{sgn}(\sigma)\prod_{i = 1}^p r_{i, \sigma_i}\Big)
	\end{align*}
	Notice that for each $\sigma \in S_p$, we have the following relationship:
	\begin{align*}
		r_{i, \sigma_i} = \frac{s_{i, \sigma_i}}{\sqrt{s_{ii}s_{\sigma_i, \sigma_i}}} \text{ or }s_{i, \sigma_i} &= r_{i, \sigma_i} \cdot (\sqrt{s_{ii}s_{\sigma_i, \sigma_i}})
	\end{align*}
	So
	\begin{align*}
	\prod_{i = 1}^p s_{i, \sigma_i} &= \prod_{i = 1}^p \Big[r_{i, \sigma_i}\cdot (\sqrt{s_{ii}s_{\sigma_i, \sigma_i}})\Big] = \Big(\prod_{i = 1}^p \sqrt{s_{ii}}\Big)\Big(\prod_{i = 1}^p \sqrt{s_{\sigma_i, \sigma_i}}\Big)\Big(\prod_{i = 1}^p r_{i, \sigma_i}\Big)
	\end{align*}
	Notice that since $\sigma$ is a permutation of $\{1, 2, \ldots, p\}$, so
	\begin{align*}
		\prod_{i = 1}^p \sqrt{s_{\sigma_i, \sigma_i}} = \prod_{i = 1}^p\sqrt{s_{ii}}
	\end{align*}
	So
	\begin{align*}
		\prod_{i = 1}^p s_{i, \sigma_i} &= \prod_{i = 1}^p \Big[r_{i, \sigma_i}\cdot (\sqrt{s_{ii}s_{\sigma_i, \sigma_i}})\Big] \\
		&= \Big(\prod_{i = 1}^p\sqrt{s_{ii}}\Big)\Big(\prod_{i = 1}^p\sqrt{s_{ii}}\Big)\Big(\prod_{i = 1}^pr_{i, \sigma_i}\Big)\\
		&= (s_{11}s_{22}\cdots s_{pp})\prod_{i = 1}^pr_{i, \sigma_i}
	\end{align*}
	Since this relationship holds true for any permutation $\sigma \in S_p$, thus we have:
	\begin{align*}
		|{\bf S}| &= \Sigma_{\sigma \in S_p}\Big(\text{sgn}(\sigma)\prod_{i = 1}^p s_{i, \sigma_i}\Big)\\
		&= \Sigma_{\sigma \in S_p}\Big(\text{sgn}(\sigma)s_{11}s_{22}\cdots s_{pp}\prod_{i = 1}^pr_{i, \sigma_i}\Big)\\
		&= (s_{11}s_{22}\ldots s_{pp})\Sigma_{\sigma \in S_p}\Big(\text{sgn}(\sigma)\prod_{i = 1}^p r_{i, \sigma_i}\Big)\\
		&= (s_{11}s_{22}\ldots s_{pp})|{\bf R}|
	\end{align*}
	Thus completed the proof of equation $(3.21)$.\vskip 2mm
	As for part $(ii)$\vskip 2mm
	 we know that for any $2\times 2$ matrix, say ${\bf U} = [{\bf u}_1, {\bf u}_2]$ as in our question (so ${\bf U}$ is a $2\times 2$ matrix whose first column is vector ${\bf u}_1$ and second column is vector ${\bf u}_2$), the area generated by ${\bf u}_1$ and ${\bf u}_2$ is the absolute value of the determinant of ${\bf U}$, which is $\Big| \text{det}({\bf U})\Big|$.\vskip 2mm
	 Similarly, given ${\bf v}_1 = {\bf A}{\bf u}_1$ and ${\bf v}_2 = {\bf A}{\bf u}_2$, we have:
	 \begin{align*}
	 	\text{Area}({\bf v}_1, {\bf v}_2) &= \Big|\text{det}(\Big[{\bf v}_1, {\bf v}_2\Big])\Big| = \Big|\text{det}(\Big[{\bf A}{\bf u}_1, {\bf A}{\bf u}_2\Big])\Big|\\
	 	&= \Big|\text{det}({\bf A}\cdot \Big[{\bf u}_1, {\bf u}_2\Big])\Big|\\
	 	&= \Big|\text{det}(A)\cdot \text{det}(\Big[{\bf u}_1, {\bf u}_2\Big])\Big|\\
	 	&= \Big|\text{det}(A)\Big| \cdot \Big|\text{det}(\Big[{\bf u}_1, {\bf u}_2\Big])\Big|\\
	 	&=  \Big|\text{det}(A)\Big| \cdot \text{Area}({\bf u}_1, {\bf u}_2)
	 \end{align*}
	 Thus completed the proof for this part.\vskip 2mm
	 For part $[B]$:\vskip 2mm
	 For part $(i)$:
	 So we have a data with $n = 5$ observatios and $p = 3$ variables(covariates).\vskip 2mm
	 Since ${\bf x}_1 =(3, 6, 4, 7, 5)'$, ${\bf x}_2 = (1, 4, 2, 0, 3)'$ and ${\bf x}_3 = (0, 6, 2, 3, 4)$, we have the sample means:
	 \begin{align*}
	 	\bar{{\bf x}}_1 &= \frac{3 + 6 + 4 + 7 + 5}{5} = 5\\
	 	\bar{{\bf x}}_2 &= \frac{1 + 4 + 2 + 0 + 3}{5} = 2\\
	 	\bar{{\bf x}}_3 &= \frac{0 + 6 + 2 + 3 + 4}{5} = 3\\
	 \end{align*}
	 Hence we have deviation vectors:
	 \begin{align*}
	 	{\bf d}_1 &= {\bf x}_1 - \bar{{\bf x}}_1{\bf 1} = (3, 6, 4, 7, 5)' - (5, 5, 5, 5, 5)' = (-2, 1, -1, 2, 0)'\\
	 	{\bf d}_2 &= {\bf x}_2 - \bar{{\bf x}}_2{\bf 1} = (1, 4, 2, 0, 3)' - (2, 2, 2, 2, 2)' = (-1, 2, 0, -2, 1)'\\
	 	{\bf d}_3 &= {\bf x}_3 - \bar{{\bf x}}_3{\bf 1} = (0, 6, 2, 3, 4)' - (3, 3, 3, 3, 3)' = (-3, 3, -1, 0, 1)'
	 \end{align*}
	 So the deviation matrix is:
	 \begin{align*}
	 	{\bf D} &= \left[{\bf d}_1, {\bf d}_2 , {\bf d}_3\right] = \left[\begin{array}{ccc}-2&-1&-3\\ 1&2&3\\ -1&0&-1\\ 2&-2&0\\0 &1&1\end{array}\right]
	 \end{align*}
	 it is obvious to see that ${\bf d}_1 + {\bf d}_2= {\bf d}_3$ so with coefficient vector ${\bf a}^T = [1, 1, -1]$ we establish the dependence among ${\bf d}_1, {\bf d}_2$ and ${\bf d}_3$.\vskip 2mm
	 For part $(ii)$:\vskip 2mm
	We are looking for sample covariance matrix ${\bf S}$ and verify the generalized variance is $0$.\vskip 2mm
	I would like to point out that, although does not affect the result for generalized variance, the book from Richard and Dean has not been consistent with the definition of ${\bf S}$. On page $118$ example $3.4$, it is computing sample variance and covariance with formulas:
	\begin{align*}
		s_{ii} &= \frac{1}{n}{\bf d}'_i{\bf d}_i = \frac{1}{n}\sum_{j = 1}^n (x_{ji} - \bar{{\bf x}_i})^2\\
		s_{ik} &= \frac{1}{n}{\bf d}'_i{\bf d}_k = \frac{1}{n}\sum_{j = 1}^n(x_{ji} - \bar{{\bf x}_i})(x_{jk} - \bar{{\bf x}_k})
	\end{align*}
	with $n = 3$ in the example.\vskip 2mm
	However on page $123$ section 3.4, it is computing $s_{ik}$ as
	\begin{align*}
		s_{ik} &= \frac{1}{n - 1}\sum_{j = 1}^{n}(x_{ji} - \bar{x}_i)(x_{jk} - \bar{x}_k)
	\end{align*}
	I would follow the second way here because it is consistent with the sample variance definition I am familiar with.\vskip 2mm
	In our example, $n = 5$ so $ n - 1 = 4$, thus:
	\begin{align*}
		s_{11} &=\frac{1}{4}{\bf d}_1'{\bf d}_1 =  \frac{1}{4}\Big((-2)^2 + 1^2 + (-1)^2 + 2^2\Big) = \frac{10}{4} = \frac{5}{2}\\
		s_{12} &= \frac{1}{4}{\bf d}_1'{\bf d}_2 = \frac{1}{4}\Big((-2)\times (-1) + 1 \times 2 + (-1)\times 0 + (2\times (-2)) + 0 \times 1\Big) = 0\\
		s_{13} &= \frac{1}{4}{\bf d}_1'{\bf d}_3 = \frac{1}{4}\Big((-2)\times(-3)+ (1\times 3)+ (-1)\times (-1) + 2\times 0 + 0 \times 1\Big) = \frac{5}{2}\\
		s_{22} &= \frac{1}{4}{\bf d}_2'{\bf d}_2 =  \frac{1}{4}\Big((-1)^2+ 2^2 + 0^2 + (-2)^2+ 1^2\Big) = \frac{5}{2}\\
		s_{23} &= \frac{1}{4}{\bf d}_2'{\bf d}_3 = \frac{1}{4}\Big((-1)\times(-3) + 2\times 3 + 0\times(-1) + (-2)\times 0 + 1 \times 1\Big) = \frac{5}{2}\\
		s_{33} &= \frac{1}{4}{\bf d}_3'{\bf d}_3 = \frac{1}{4}\Big((-3)^2 + 3^2 + (-1)^2 + 0^2 + 1^2\Big) = 5
	\end{align*}
	So the sample covariance matrix is:
	\begin{align*}
		{\bf S} &= \left[\begin{array}{ccc}\frac{5}{2}&0&\frac{5}{2}\\ 0&\frac{5}{2}&\frac{5}{2}\\\frac{5}{2} &\frac{5}{2}&5\end{array}\right]
	\end{align*}
	Notice that the for $S$, the third column is the sum of first and second column, so ${\bf S}$ is not of full rank(or in other words, ${\bf S}$ is singular), and hence the generalized variance is:
	\begin{align*}
		\Big|{\bf S}\Big| &= 0
	\end{align*}
	For part $(iii)$\vskip 2mm
	To show that the columns of the data matrix are linearly independent, it suffice to show that the rank of the data matrix is $3$, and since we have three columns, that complete the proof.\vskip 2mm
	To show that, we can do Gauss elimination to get the echelon matrix, and we will see the echelon matrix has rank $3$.
	\begin{align*}
		\left[\begin{array}{ccc} 3&1&0 \\ 6&4&6\\ 4&2&2\\ 7&0&3\\ 5&3&4\end{array}\right] \Longrightarrow \left[\begin{array}{ccc} 3&1& 0\\ 0&2&6\\ 0&\frac{2}{3}&2\\ 0&-\frac{7}{3}&3\\ 0&\frac{4}{3}&4\end{array}\right] \Longrightarrow \left[\begin{array}{ccc} 3&1&0 \\ 0&1&3\\ 0&\frac{2}{3}&2\\ 0&-\frac{7}{3}&3\\ 0&\frac{4}{3}&4\end{array}\right]\Longrightarrow \left[\begin{array}{ccc} 3&1&0 \\ 0&1&3\\ 0&0&0\\ 0&0&10\\ 0&0&0\end{array}\right]
	\end{align*}
	The elimination above is all standard row elimination process, and as we can see there are three rows left so the rank of data matrix is $3$, and hence the three columns are linearly independent.
\end{sol}
\vskip 2mm
Question $\# 2$.
\begin{sol}
	For part $[A]$:\vskip 2mm
	For part $(i)$:\vskip 2mm
	We want to prove the following equation by using Theorem $2.9c$.
	\begin{align*}
		\Big|\begin{array}{cc} {\bf A}_{11}& {\bf 0}\\ {\bf 0}& {\bf A}_{22}\end{array}\Big| = |{\bf A}_{11}|\cdot |{\bf A}_{22}|
	\end{align*}
	Notice that
	\begin{align*}
		&\ \left[\begin{array}{cc} {\bf A}_{11}& {\bf 0}\\ {\bf 0}& {\bf A}_{22}\end{array}\right] = \left[\begin{array}{cc} {\bf A}_{11}& {\bf 0}\\ {\bf 0}& {\bf I}\end{array}\right]\cdot \left[\begin{array}{cc}{\bf I}& {\bf 0}\\ {\bf 0}& {\bf A}_{22}\end{array}\right]
	\end{align*}
	So
	\begin{align*}
		\Big|\left[\begin{array}{cc} {\bf A}_{11}& {\bf 0}\\ {\bf 0}& {\bf A}_{22}\end{array}\right]\Big| = \Big|\left[\begin{array}{cc} {\bf A}_{11}& {\bf 0}\\ {\bf 0}& {\bf I}\end{array}\right]\Big|\cdot \Big|\left[\begin{array}{cc}{\bf I}& {\bf 0}\\ {\bf 0}& {\bf A}_{22}\end{array}\right]\Big|
	\end{align*}
	We just need to show that
	\begin{align*}
		 \Big|\left[\begin{array}{cc} {\bf A}_{11}& {\bf 0}\\ {\bf 0}& {\bf I}\end{array}\right]\Big| &= |{\bf A}_{11}|\hskip 1cm \text{ and }\hskip 1cm \Big|\left[\begin{array}{cc}{\bf I}& {\bf 0}\\ {\bf 0}& {\bf A}_{22}\end{array}\right]\Big| = |{\bf A}_{22}|
	\end{align*}
	We could not directly claim the above is true without using Theorem 2.9b. The question here requires only using Theorem 2.9c, so that needs justification.\vskip 2mm
	It would suffice to show the first one because the proof of second one is very similar.\vskip 2mm
	Assume that ${\bf A}_{11}$ is an $r \times r$ matrix, and denote by $\sigma$ the permutation of $\{1, 2, \ldots, n\}$ and $S_n$ the set of permutations, also denote $\text{sgn}(\sigma)$ the signature of $\sigma$ as we did in part $[A]$\vskip 2mm
	Also, in the proof below we abuse the notation a little bit and use $a_{ij}$ to denote the $(i, j)$ entry of 
	\begin{align*}
		\left[\begin{array}{cc} {\bf A}_{11}& {\bf 0}\\ {\bf 0}& {\bf I}\end{array}\right]
	\end{align*}
	and specifically, when $1 \leq i \leq r$ and $1 \leq j \leq r$, $a_{ij}$ is the $(i, j)$ entry of ${\bf A}_{11}$.
	Then we have by Leibniz formula:
	\begin{align*}
		 \Big|\left[\begin{array}{cc} {\bf A}_{11}& {\bf 0}\\ {\bf 0}& {\bf I}\end{array}\right]\Big| &= \sum_{\sigma \in S_n}\Big(\text{sgn}(\sigma)\prod_{i = 1}^n a_{i, \sigma_i}\Big)
	\end{align*}
	However since the lower right corner is identity matrix ${\bf I}$, so $\prod_{i = 1}^n a_{i, \sigma_i} \neq 0$ if and only if $a_{i, \sigma_i} = 1$ for $i = r + 1, r+2, \ldots, n$, or equivalently, if and only if $\sigma_{i} = i$ for $i = r+1, r+2, \ldots, n$. Thus $\sigma$ is really just a permutation of the first $r$ integers $\{1, 2, \ldots, r\}$, so $\sigma \in S_r$, and we have:
	\begin{align*}
		\left[\begin{array}{cc} {\bf A}_{11}& {\bf 0}\\ {\bf 0}& {\bf I}\end{array}\right] = \sum_{\sigma \in S_r}\Big(\text{sgn}(\sigma)\prod_{i = 1}^r a_{i, \sigma_i}\Big) = |{\bf A}_{11}|
	\end{align*}
	Similar proof follows for the other matrix. Thus we have proved that
	\begin{align*}
		\Big|\begin{array}{cc} {\bf A}_{11}& {\bf 0}\\ {\bf 0}& {\bf A}_{22}\end{array}\Big| = |{\bf A}_{11}|\cdot |{\bf A}_{22}|
	\end{align*}
	\vskip 2mm
	For part $[B]$:\vskip 2mm
	Notice that:
	\begin{align*}
		&\ \left[\begin{array}{cc} {\bf I}&-{\bf A}_{12}{\bf A}_{22}^{-1} \\ {\bf 0}^T &{\bf I} \end{array}\right]\cdot \left[\begin{array}{cc} {\bf A}_{11}&{\bf A}_{12} \\ {\bf A}_{21}&{\bf A}_{22} \end{array}\right]\cdot \left[\begin{array}{cc} {\bf I}&{\bf 0} \\{\bf -A}_{22}^{-1}{\bf A}_{21} &{\bf I} \end{array}\right] \\
		&= \left[\begin{array}{cc} {\bf A}_{11}- {\bf A}_{12}{\bf A}_{22}^{-1}{\bf A}_{21}&{\bf 0} \\ {\bf A}_{21}&{\bf A}_{22} \end{array}\right]\cdot \left[\begin{array}{cc} {\bf I}& {\bf 0}\\ -{\bf A}_{22}^{-1}{\bf A}_{21}&{\bf I} \end{array}\right]\\
		&= \left[\begin{array}{cc} {\bf A}_{11} - {\bf A}_{12}{\bf A}_{22}^{-1}{\bf A}_{21}& {\bf 0}\\ {\bf 0}&{\bf A}_{22} \end{array}\right]
	\end{align*}	
	By the results from both $[A]$ and $[B]$, we have:
	\begin{align*}
		&\ \Big|\left[\begin{array}{cc} {\bf I}&-{\bf A}_{12}{\bf A}_{22}^{-1} \\ {\bf 0}^T &{\bf I} \end{array}\right]\cdot \left[\begin{array}{cc} {\bf A}_{11}&{\bf A}_{12} \\ {\bf A}_{21}&{\bf A}_{22} \end{array}\right]\cdot \left[\begin{array}{cc} {\bf I}&{\bf 0} \\{\bf -A}_{22}^{-1}{\bf A}_{21} &{\bf I} \end{array}\right]\Big|\\
		&= \Big|\left[\begin{array}{cc} {\bf I}&-{\bf A}_{12}{\bf A}_{22}^{-1} \\ {\bf 0}^T &{\bf I} \end{array}\right]\Big|\cdot \Big|\left[\begin{array}{cc} {\bf A}_{11}&{\bf A}_{12} \\ {\bf A}_{21}&{\bf A}_{22} \end{array}\right]\Big|\cdot \Big| \left[\begin{array}{cc} {\bf I}&{\bf 0} \\{\bf -A}_{22}^{-1}{\bf A}_{21} &{\bf I} \end{array}\right]\Big|\\
		&= 1 \cdot  \Big|\left[\begin{array}{cc} {\bf A}_{11}&{\bf A}_{12} \\ {\bf A}_{21}&{\bf A}_{22} \end{array}\right]\Big|\cdot 1\\
		&= \Big|  \left[\begin{array}{cc} {\bf A}_{11} - {\bf A}_{12}{\bf A}_{22}^{-1}{\bf A}_{21}& {\bf 0}\\ {\bf 0}&{\bf A}_{22} \end{array}\right]\Big|\\
		&=|{\bf A}_{22}|\cdot |{\bf A}_{11} - {\bf A}_{12}{\bf A}_{22}^{-1}{\bf A}_{21}|
	\end{align*}
	Similarly, notice that:
	\begin{align*}
		&\ \left[\begin{array}{cc} {\bf I}& {\bf 0}\\ -{\bf A}_{21}{\bf A}_{11}^{-1}& {\bf I}\end{array}\right]\cdot \left[\begin{array}{cc} {\bf A}_{11}& {\bf A}_{12}\\ {\bf A}_{21}&{\bf A}_{22} \end{array}\right]\cdot \left[\begin{array}{cc} {\bf I}& -{\bf A}_{11}^{-1}{\bf A}_{12}\\ {\bf 0}& {\bf I}\end{array}\right] \\
		&= \left[\begin{array}{cc} {\bf A}_{11}& {\bf A}_{12}\\ {\bf 0}& {\bf A}_{22}- {\bf A}_{21}{\bf A}_{11}^{-1}{\bf A}_{12}\end{array}\right]\cdot \left[\begin{array}{cc} {\bf I}& -{\bf A}_{11}^{-1}{\bf A}_{12}\\ {\bf 0}& {\bf I}\end{array}\right]\\
		&= \left[\begin{array}{cc}{\bf A}_{11}& {\bf 0}\\ {\bf 0}& {\bf A}_{22} - {\bf A}_{21}{\bf A}_{11}^{-1}{\bf A}_{12}\end{array}\right]
	\end{align*}
	So we have:
	\begin{align*}
		&\ \Big|\left[\begin{array}{cc} {\bf I}& {\bf 0}\\ -{\bf A}_{21}{\bf A}_{11}^{-1}& {\bf I}\end{array}\right]\cdot \left[\begin{array}{cc} {\bf A}_{11}& {\bf A}_{12}\\ {\bf A}_{21}&{\bf A}_{22} \end{array}\right]\cdot \left[\begin{array}{cc} {\bf I}& -{\bf A}_{11}^{-1}{\bf A}_{12}\\ {\bf 0}& {\bf I}\end{array}\right] \Big|\\
		&= \Big|\left[\begin{array}{cc} {\bf I}& {\bf 0}\\ -{\bf A}_{21}{\bf A}_{11}^{-1}& {\bf I}\end{array}\right]\Big|\cdot \Big|\left[\begin{array}{cc} {\bf A}_{11}& {\bf A}_{12}\\ {\bf A}_{21}&{\bf A}_{22} \end{array}\right]\Big|\cdot \Big|\left[\begin{array}{cc} {\bf I}& -{\bf A}_{11}^{-1}{\bf A}_{12}\\ {\bf 0}& {\bf I}\end{array}\right]\Big|\\
		&= 1\cdot \Big|\left[\begin{array}{cc} {\bf A}_{11}& {\bf A}_{12}\\ {\bf A}_{21}&{\bf A}_{22} \end{array}\right]\Big|\cdot 1\\
		&= \Big| \left[\begin{array}{cc}{\bf A}_{11}& {\bf 0}\\ {\bf 0}& {\bf A}_{22} - {\bf A}_{21}{\bf A}_{11}^{-1}{\bf A}_{12}\end{array}\right]\Big|\\
		&= |{\bf A}_{11}|\cdot |{\bf A}_{22} - {\bf A}_{21}{\bf A}_{11}^{-1}{\bf A}_{12}|
	\end{align*}
	Thus completed the proof of equation of both $(2.71)$ and $(2.72)$.\vskip 2mm
	For part $[C]$:\vskip 2mm
	We want to show that:
	\begin{align*}
		{\bf A}^{-1}= \left[\begin{array}{cc} {\bf I}& {\bf 0}\\ -{\bf A}_{22}^{-1}{\bf A}_{21} &{\bf I}  \end{array}\right]\left[\begin{array}{cc} ({\bf A}_{11}- {\bf A}_{12}{\bf A}_{22}^{-1}{\bf A}_{21})^{-1}& {\bf 0}\\ {\bf 0}^T& {\bf A}_{22}^{-1}\end{array}\right]\left[\begin{array}{cc} {\bf I}& -{\bf A}_{12}{\bf A}_{22}^{-1}\\ {\bf 0}^T& {\bf I}\end{array}\right]
	\end{align*}
	We just need to show that the right hand side multiply by ${\bf A}$ gives identity matrix.\vskip 2mm
	We have:
	\begin{align*}
		&\ \left[\begin{array}{cc} {\bf I}& {\bf 0}\\ -{\bf A}_{22}^{-1}{\bf A}_{21} &{\bf I}  \end{array}\right]\left[\begin{array}{cc} ({\bf A}_{11}- {\bf A}_{12}{\bf A}_{22}^{-1}{\bf A}_{21})^{-1}& {\bf 0}\\ {\bf 0}^T& {\bf A}_{22}^{-1}\end{array}\right]\left[\begin{array}{cc} {\bf I}& -{\bf A}_{12}{\bf A}_{22}^{-1}\\ {\bf 0}^T& {\bf I}\end{array}\right]\left[\begin{array}{cc}{\bf A}_{11}& {\bf A}_{12}\\ {\bf A}_{21}& {\bf A}_{22}\end{array}\right]\\
		&= \Big(\left[\begin{array}{cc} {\bf I}& {\bf 0}\\ -{\bf A}_{22}^{-1}{\bf A}_{21} &{\bf I}  \end{array}\right]\left[\begin{array}{cc} ({\bf A}_{11}- {\bf A}_{12}{\bf A}_{22}^{-1}{\bf A}_{21})^{-1}& {\bf 0}\\ {\bf 0}^T& {\bf A}_{22}^{-1}\end{array}\right]\Big)\Big(\left[\begin{array}{cc} {\bf I}& -{\bf A}_{12}{\bf A}_{22}^{-1}\\ {\bf 0}^T& {\bf I}\end{array}\right]\left[\begin{array}{cc}{\bf A}_{11}& {\bf A}_{12}\\ {\bf A}_{21}& {\bf A}_{22}\end{array}\right]\Big)\\
		&= \left[\begin{array}{cc} ({\bf A}_{11} - {\bf A}_{12}{\bf A}_{22}^{-1}{\bf A}_{21})^{-1}& {\bf 0}\\ -{\bf A}_{22}^{-1}{\bf A}_{21}({\bf A}_{11} - {\bf A}_{12}{\bf A}_{22}^{-1}{\bf A}_{21})^{-1}& {\bf A}_{22}^{-1}\end{array}\right] \left[\begin{array}{cc} {\bf A}_{11} - {\bf A}_{12}{\bf A}_{22}^{-1}{\bf A}_{21}&{\bf 0} \\ {\bf A}_{21}& {\bf A}_{22}\end{array}\right]\\
		&= \left[\begin{array}{cc} {\bf I}& {\bf 0}\\ -{\bf A}_{22}^{-1}{\bf A}_{21} + {\bf A}_{22}^{-1}{\bf A}_{21}& {\bf I}\end{array}\right]\\
		&= \left[\begin{array}{cc} {\bf I}& {\bf 0}\\ {\bf 0}& {\bf I}\end{array}\right]
	\end{align*}
	thus completed the proof.
\end{sol}

Question $\# 3.$
\begin{sol}
	For part $[A]$:\vskip 2mm
	with 
	\begin{align*}
		{\bf \mu} = \left[\begin{array}{c}3\\ 4\\ -1\\ -2\end{array}\right]\hskip 1cm \text{ and }\Sigma = \left[\begin{array}{cccc}3&-4&2&1\\ -4&4&-3&-1\\ 2&-3&2&6\\ 1&-1&6&5\end{array}\right]
	\end{align*}
	We work on the following problems:\vskip 2mm
	part $(a)$:\vskip 2mm
	The joint marginal distribution of $y_{1}$ and $y_3$:\vskip 2mm
	It is straightforward to see that:
	\begin{align*}
		\left[\begin{array}{c} y_1\\ y_3\end{array}\right] \text{ is } N_2(\left[\begin{array}{c}3 \\ -1 \end{array}\right], \left[\begin{array}{cc}3& 2\\ 2& 2\end{array}\right])
	\end{align*}
	part $(b)$: \vskip 2mm
	The marginal distribution of $y_2$:\vskip 2mm
	The marginal distribution of $y_2$ can simply be read from the joint distribution of ${\bf y}$, and $y_2$ is $N(4, 4)$\vskip 2mm
	part $(c)$:\vskip 2mm
	The distribution of $z = y_1 + 2y_2 - y_3 + 3y_4$\vskip 2mm
	Notice that $z = {\bf a}'{\bf y}$ where ${\bf a} = (1, 2, -1, 3)'$.\vskip 2mm
	So we know that:
	\begin{align*}
		z \text{ is } N({\bf a}'{\bf \mu}, {\bf a}'\Sigma {\bf a})
	\end{align*}
	In this case, we have:
	\begin{align*}
		{\bf a}'{\bf \mu} = (1, 2, -1, 3)\left[\begin{array}{c}3\\ 4\\ -1\\ -2\end{array}\right] = 6
	\end{align*}
	and
	\begin{align*}
		{\bf a}'\Sigma {\bf a} &= (1, 2, -1, 3)\left[\begin{array}{cccc} 3&-4&2& 1\\ -4&4&-3&-1 \\ 2&-3&2&6\\ 1&-1&6&5 \end{array}\right]\left[\begin{array}{c}1\\ 2\\ -1\\ 3\end{array}\right]\\
		&= (-4, 4, 12, 8)\left[\begin{array}{c}1\\ 2\\ -1\\ 3\end{array}\right] = 16
	\end{align*}
	So $z$ is $N(6, 16)$.\vskip 2mm
	part $(d)$:\vskip 2mm
	The joint distribution of $z_1 = y_1 + y_2 - y_3 - y_4$ and $z_2 = -3y_1 + y_2 + 2y_3 - 2y_4$:\vskip 2mm
	So we have:
	\begin{align*}
		\left[\begin{array}{c} z_1\\ z_2\end{array}\right] &= \left[\begin{array}{cccc}1&1&-1&-1\\ -3&1&2&-2 \end{array}\right]\cdot \left[\begin{array}{c} y_1\\ y_2\\ y_3\\ y_4\end{array}\right]
	\end{align*}
	If we denote:
	\begin{align*}
		{\bf A} &= \left[\begin{array}{cccc}1&1&-1&-1\\ -3&1&2&-2 \end{array}\right]
	\end{align*}	
	Then
	\begin{align*}
		\left[\begin{array}{c} z_1\\ z_2\end{array}\right] \text{ is } N_2({\bf A}{\bf \mu}, {\bf A}\Sigma{\bf A}')
	\end{align*}
	We have:
	\begin{align*}
		{\bf A}{\bf \mu} &= \left[\begin{array}{cccc}1&1&-1&-1\\ -3&1&2&-2 \end{array}\right]\left[\begin{array}{c} 3\\ 4\\ -1\\ -2\end{array}\right] = \left[\begin{array}{c}10 \\ -3\end{array}\right]
	\end{align*}
	and
	\begin{align*}
		{\bf A}\Sigma{\bf A}' &= \left[\begin{array}{cccc}1&1&-1&-1\\ -3&1&2&-2 \end{array}\right]\left[\begin{array}{cccc} 3&-4&2& 1\\ -4&4&-3&-1 \\ 2&-3&2&6\\ 1&-1&6&5 \end{array}\right]\left[\begin{array}{cc} 1&-3 \\ 1& 1\\ -1&2 \\ -1& -2\end{array}\right]\\
		&= \left[\begin{array}{cccc} -4&4&-9&-11 \\ -11&12&-17&-2 \end{array}\right]\left[\begin{array}{cc} 1&-3 \\ 1& 1\\ -1&2 \\ -1& -2\end{array}\right]\\
		&= \left[\begin{array}{cc} 20& 20\\ 20& 15\end{array}\right]
	\end{align*}
	So we have:
	\begin{align*}
		\left[\begin{array}{c} z_1\\ z_2\end{array}\right] \text{ is } N_2(\left[\begin{array}{c}10 \\ -3\end{array}\right], \left[\begin{array}{cc} 20& 20\\ 20& 15\end{array}\right])
	\end{align*}
	part $(e)$:\vskip 2mm
	we are looking for $f(y_1, y_2|y_3, y_4)$\vskip 2mm
	Let's denote by:
	\begin{align*}
		{\bf y} &= \left[\begin{array}{c} y_1\\ y_2\end{array}\right]\\
		{\bf x} &= \left[\begin{array}{c} y_3\\ y_4\end{array}\right]
	\end{align*}
	Then we have:
	\begin{align*}
		{\bf \mu}_y &=  \left[\begin{array}{c} 3\\ 4\end{array}\right]\\
		{\bf \mu}_x &=  \left[\begin{array}{c} -1\\ -2\end{array}\right]\\
		{\bf \Sigma}_{yy} &= \left[\begin{array}{cc} 3&-4 \\ -4& 4\end{array}\right]\\
		{\bf \Sigma}_{yx} &= \left[\begin{array}{cc} 2& 1\\ -3&-1 \end{array}\right]\\
		{\bf \Sigma}_{xy} &= \left[\begin{array}{cc}2 &-3 \\ 1& -1\end{array}\right]\\
		{\bf \Sigma}_{xx} &= \left[\begin{array}{cc} 2& 6\\ 6&5 \end{array}\right]
	\end{align*}
	So $f(y_1, y_2|y_3, y_4)$ is multivariate normal with mean vector and covariance matrix:
	\begin{align*}
		E[\left[\begin{array}{c} y_1\\ y_2\end{array}\right]|\left[\begin{array}{c} y_3\\ y_4\end{array}\right]] &= {\bf \mu}_y + \Sigma_{yx}\Sigma_{xx}^{-1}({\bf x} - {\bf \mu}_x)\\
		&= \left[\begin{array}{c} 3\\ 4\end{array}\right] + \left[\begin{array}{cc} 2& 1\\ -3&-1 \end{array}\right]\left[\begin{array}{cc} -\frac{5}{26}& \frac{3}{13}\\ \frac{3}{13}&-\frac{1}{13} \end{array}\right](\left[\begin{array}{c} y_3\\ y_4\end{array}\right] - \left[\begin{array}{c} -1\\ -2\end{array}\right])\\
		&= \left[\begin{array}{c} 3\\ 4\end{array}\right] +  \left[\begin{array}{cc}-\frac{2}{13} & \frac{5}{13}\\ \frac{9}{26}&-\frac{8}{13} \end{array}\right](\left[\begin{array}{c} y_3\\ y_4\end{array}\right] - \left[\begin{array}{c} -1\\ -2\end{array}\right])\\
		&= \left[\begin{array}{c} -\frac{2}{13}y_3 + \frac{5}{13}y_4 + \frac{47}{13}\\ \frac{9}{26}y_3 - \frac{8}{13}y_4 + \frac{81}{26} \end{array}\right]
	\end{align*}
	\begin{align*}
		\text{cov}(\left[\begin{array}{c} y_1\\ y_2\end{array}\right]|\left[\begin{array}{c} y_3\\ y_4\end{array}\right]) &= \Sigma_{yy} - \Sigma_{yx}\Sigma_{xx}^{-1}\Sigma_{xy}\\
		&=  \left[\begin{array}{cc} 3&-4 \\ -4& 4\end{array}\right] - \left[\begin{array}{cc} 2& 1\\ -3&-1 \end{array}\right]\left[\begin{array}{cc} 2& 6\\ 6&5 \end{array}\right]^{-1}\left[\begin{array}{cc}2 &-3 \\ 1& -1\end{array}\right]\\
		&=  \left[\begin{array}{cc} 3&-4 \\ -4& 4\end{array}\right] - \left[\begin{array}{cc} 2& 1\\ -3&-1 \end{array}\right]\left[\begin{array}{cc} -\frac{5}{26}& \frac{3}{13}\\ \frac{3}{13}&-\frac{1}{13} \end{array}\right]\left[\begin{array}{cc}2 &-3 \\ 1& -1\end{array}\right]\\
		&=\left[\begin{array}{cc} 3&-4 \\ -4& 4\end{array}\right]  - \left[\begin{array}{cc} \frac{1}{13}& \frac{1}{13}\\ \frac{1}{13}& -\frac{11}{26}\end{array}\right]\\
		&= \left[\begin{array}{cc} \frac{38}{13}& -\frac{53}{13}\\-\frac{53}{13} &\frac{115}{26} \end{array}\right]\\
		&= \left[\begin{array}{cc}2.923077& -4.076923\\ -4.076923& 4.423077\end{array}\right]
	\end{align*}
	part $(f)$:\vskip 2mm
	we are looking for $f(y_1, y_3|y_2, y_4)$\vskip 2mm
	Let's denote by
	\begin{align*}
		{\bf y} &= \left[\begin{array}{c} y_1\\ y_3\end{array}\right]\\
		{\bf x} &= \left[\begin{array}{c} y_2\\ y_4\end{array}\right]
	\end{align*}
	Then we have:
	\begin{align*}
		{\bf \mu}_y &=  \left[\begin{array}{c} 3\\ -1\end{array}\right]\\
		{\bf \mu}_x &=  \left[\begin{array}{c} 4\\ -2\end{array}\right]\\
		{\bf \Sigma}_{yy} &= \left[\begin{array}{cc} 3&2 \\ 2& 2\end{array}\right]\\
		{\bf \Sigma}_{yx} &= \left[\begin{array}{cc} -4& 1\\ -3&6 \end{array}\right]\\
		{\bf \Sigma}_{xy} &= \left[\begin{array}{cc}-4 &-3 \\ 1& 6\end{array}\right]\\
		{\bf \Sigma}_{xx} &= \left[\begin{array}{cc} 4& -1\\ -1&5 \end{array}\right]
	\end{align*}
	So $f(y_1, y_3|y_2, y_4)$ is multivariate normal with mean vector and covariance matrix:
	\begin{align*}
		E[\left[\begin{array}{c} y_1\\ y_3\end{array}\right]|\left[\begin{array}{c} y_2\\ y_4\end{array}\right]] &= {\bf \mu}_y + \Sigma_{yx}\Sigma_{xx}^{-1}({\bf x} - {\bf \mu}_x)\\
		&=\left[\begin{array}{c} 3\\ -1\end{array}\right] + \left[\begin{array}{cc} -4& 1\\ -3&6 \end{array}\right]\left[\begin{array}{cc} 4& -1\\ -1&5 \end{array}\right]^{-1}(\left[\begin{array}{c} y_2\\ y_4\end{array}\right] - \left[\begin{array}{c} 4\\ -2\end{array}\right])\\
		&= \left[\begin{array}{c} 3\\ -1\end{array}\right] + \left[\begin{array}{cc} -4& 1\\ -3&6 \end{array}\right]\left[\begin{array}{cc}\frac{5}{19} & \frac{1}{19}\\ \frac{1}{19}& \frac{4}{19}\end{array}\right](\left[\begin{array}{c} y_2\\ y_4\end{array}\right] - \left[\begin{array}{c} 4\\ -2\end{array}\right])\\
		&= \left[\begin{array}{c} 3\\ -1\end{array}\right] + \left[\begin{array}{cc}-1&0 \\ -\frac{9}{19}&\frac{21}{19}  \end{array}\right](\left[\begin{array}{c} y_2\\ y_4\end{array}\right] - \left[\begin{array}{c} 4\\ -2\end{array}\right])\\
		&= \left[\begin{array}{c} -y_2 + 7\\ -\frac{9}{19}y_2 + \frac{21}{19}y_4 + \frac{59}{19}\end{array}\right]
	\end{align*}	
	\begin{align*}
		\text{cov}(\left[\begin{array}{c} y_1\\ y_3\end{array}\right]|\left[\begin{array}{c} y_2\\ y_4\end{array}\right]) &= \Sigma_{yy} - \Sigma_{yx}\Sigma_{xx}^{-1}\Sigma_{xy}\\
		&=  \left[\begin{array}{cc} 3&2 \\ 2& 2\end{array}\right] -  \left[\begin{array}{cc} -4& 1\\ -3&6 \end{array}\right]\left[\begin{array}{cc}\frac{5}{19} & \frac{1}{19}\\ \frac{1}{19}& \frac{4}{19}\end{array}\right] \left[\begin{array}{cc}-4 &-3 \\ 1& 6\end{array}\right]\\
		&= \left[\begin{array}{cc} 3&2 \\ 2& 2\end{array}\right]  - \left[\begin{array}{cc}-1&0 \\-\frac{9}{19}& \frac{21}{19} \end{array}\right]\left[\begin{array}{cc}-4 &-3 \\ 1& 6\end{array}\right]\\
		&=  \left[\begin{array}{cc} 3&2 \\ 2& 2\end{array}\right] - \left[\begin{array}{cc}4& 3\\ 3& \frac{153}{19}\end{array}\right]\\
		&= \left[\begin{array}{cc}-1& -1\\ -1& -\frac{115}{19}\end{array}\right]
	\end{align*}
	I found it is a bit odd that there is negative value on the diagonal entries of conditional covariance matrix. At first I thought I made a calculation mistake somewhere, but I could not find any after double check. Then I checked the updated variance-covariance matrix given by the question and found that $\Sigma$ has two negative eigenvalues $-0.4613693$ and $-3.1477987$ thus fail to be positive definite. \vskip 2mm
	part $(g)$:\vskip 2mm
	We have:
	\begin{align*}
		\text{s}_{11} &=3 \\
		\text{s}_{33} &= 2\\
		\text{s}_{13} &= 2
	\end{align*}
	and hence
	\begin{align*}
		\rho_{13} &= \frac{s_{13}}{\sqrt{s_{11}}\sqrt{s_{33}}} = \frac{2}{\sqrt{3}\sqrt{2}} = \frac{2}{\sqrt{6}} = 0.8164966
	\end{align*}
	part $(h)$:\vskip 2mm
	we have from part $(f)$ that:
	\begin{align*}
		s_{13\cdot 24} = -1 \hskip 1cm s_{11\cdot 24} = -1 \hskip 1cm s_{33\cdot 24} = \frac{-115}{19}
	\end{align*}
	So 
	\begin{align*}
		\rho_{13\cdot 24} &= \frac{s_{13\cdot 24}}{\sqrt{s_{11\cdot 24}}\sqrt{s_{33\cdot 24}}}\\
		&= \frac{-1}{\sqrt{-1}\cdot \sqrt{-\frac{115}{19}}} \hskip 1cm \text{Oops!}
	\end{align*}
	For part $(i)$:\vskip 2mm
	denote by:
	\begin{align*}
		{\bf y} = y_1, \hskip 1cm{\bf x} = \left[\begin{array}{c} y_2\\ y_3\\ y_4\end{array}\right]
	\end{align*}
	We have
	\begin{align*}
		{\bf \mu}_y = 3, \hskip 1cm {\bf \mu}_x =  \left[\begin{array}{c} 4\\ -1\\ -2\end{array}\right]
	\end{align*}
	and
	\begin{align*}
		\Sigma_{yy} = 3, \hskip 1cm \Sigma_{yx} = (-4, 2, 1) \hskip 1cm \Sigma_{xx} = \left[\begin{array}{ccc} 4&-3&-1\\ -3&2&6\\ -1&6&5\end{array}\right]
	\end{align*}
	Thus $f(y_1|y_2, y_3, y_4) = f(y|{\bf x})$ is multivariate normal with the following mean and covariance matrix:
	\begin{align*}
		E[{\bf y}|{\bf x}] &= {\bf \mu}_y  + \Sigma_{yx}\Sigma_{xx}^{-1}({\bf x} - {\bf \mu}_x)\\
		&= 3 + (-4, 2, 1)\left[\begin{array}{ccc} 4&-3&-1\\ -3&2&6\\ -1&6&5\end{array}\right]^{-1}\Big(\left[\begin{array}{c} y_2\\ y_3\\ y_4\end{array}\right] - \left[\begin{array}{c} 4\\ -1\\ -2\end{array}\right]\Big)\\
		&= 3 + (-4, 2, 1)\left[\begin{array}{ccc} \frac{26}{115}& -\frac{9}{115}&\frac{16}{115}\\ -\frac{9}{115}&-\frac{19}{115} &\frac{21}{115}\\ \frac{16}{115}&\frac{21}{115} &\frac{1}{115}\end{array}\right]\Big(\left[\begin{array}{c} y_2\\ y_3\\ y_4\end{array}\right] - \left[\begin{array}{c} 4\\ -1\\ -2\end{array}\right]\Big)\\
		&= 3 + (-\frac{106}{115}, \frac{19}{115}, -\frac{21}{115})\Big(\left[\begin{array}{c} y_2\\ y_3\\ y_4\end{array}\right] - \left[\begin{array}{c} 4\\ -1\\ -2\end{array}\right]\Big)\\
		&= 3 + \Big(-\frac{106}{115}y_2 + \frac{19}{115}y_3 - \frac{21}{115}y_4\Big) + \frac{401}{115}\\
		&=  -\frac{106}{115}y_2 + \frac{19}{115}y_3 - \frac{21}{115}y_4 + \frac{746}{115}
	\end{align*}
	and for covariance matrix, we have:
	\begin{align*}
		\text{cov}[{\bf y}|{\bf x}] &= \Sigma_{yy} - \Sigma_{yx}\Sigma_{xx}^{-1}\Sigma_{xy}\\
		&= 3 - (-4, 2, 1)\left[\begin{array}{ccc} 4&-3&-1\\ -3&2&6\\ -1&6&5\end{array}\right]^{-1}\left[\begin{array}{c} -4\\ 2\\ 1\end{array}\right]\\
		&= 3 - (-4, 2, 1)\left[\begin{array}{ccc} \frac{26}{115}& -\frac{9}{115}&\frac{16}{115}\\ -\frac{9}{115}&-\frac{19}{115} &\frac{21}{115}\\ \frac{16}{115}&\frac{21}{115} &\frac{1}{115}\end{array}\right]\left[\begin{array}{c} -4\\ 2\\ 1\end{array}\right]\\
	&= 3 - (-\frac{106}{115}, \frac{19}{115}, -\frac{21}{115})\left[\begin{array}{c} -4\\ 2\\1 \end{array}\right]\\
	&= 3 - \frac{441}{115}\\
	&= -\frac{96}{115}
	\end{align*}
	Thus completed the solution for part $[A]$.
	\vskip 2mm
	For part $[B]$:\vskip 2mm
	To find the MLE, let's first compute a few necessary components:\vskip 2mm
	Given sample:
	\begin{align*}
		{\bf X} = \left[\begin{array}{cc} 3&6  \\  4& 4\\ 5&7 \\ 4&7  \end{array}\right]
	\end{align*}
	we have:
	\begin{align*}
		\bar{{\bf x}} = \left[\begin{array}{c} \bar{x}_1\\ \bar{x}_2\end{array}\right] &= \bar{{\bf x}} = \left[\begin{array}{c} \frac{3 + 4 + 5 + 4}{4}\\ \frac{6+4+7+7}{4}\end{array}\right] = \left[\begin{array}{c} 4\\ 6\end{array}\right]
	\end{align*}
	and also:
	\begin{align*}
		s_{11} &= \frac{1}{4}\sum_{i = 1}^4(x_{i1} - \bar{x}_1)^2 = \frac{1}{2}\\
		s_{12} &= \frac{1}{4}\sum_{i = 1}^4(x_{i1} - \bar{x}_1)(x_{i2} - \bar{x}_2) = \frac{1}{4} \\
		s_{21} &= s_{12} = \frac{1}{4}\\
		s_{22} &= \frac{1}{4}\sum_{i = 1}^4(x_{i2} - \bar{x}_2)^2 = \frac{3}{2}
	\end{align*}
	So the MLE for ${\bf \mu}$ and $\Sigma$ is:
	\begin{align*}
		\hat{{\bf \mu}} &=\bar{{\bf x}} =  \left[\begin{array}{c} 4\\ 6\end{array}\right] \\
		\hat{\Sigma} &= \left[\begin{array}{cc}s_{11}&s_{12} \\s_{21}&s_{22} \end{array}\right] =  \left[\begin{array}{cc}\frac{1}{2}&\frac{1}{4} \\ \frac{1}{4}&\frac{3}{2} \end{array}\right] 
	\end{align*}
	For part $[C]$:\vskip 2mm
	{\bf The first method for testing bivariate normality:}\vskip 2mm
	Mardia's multivariate normality test:\vskip 2mm
	Mardia (1970) proposed a multivariate normality test which is based on multivariate extensions of skewness $(\hat{\gamma}_{1, p})$ and kurtosis $(\hat{\gamma}_{2, p})$ measures as follows:
	\begin{align*}
		\hat{\gamma}_{1, p} = \frac{1}{n^2}\sum_{i = 1}^{n}\sum_{j = 1}^nm_{ij}^3 \hskip 1cm \text{ and } \hat{\gamma}_{2, p} = \frac{1}{n}\sum_{i = 1}^{n}m_{ii}^2
	\end{align*}
	Here
	\begin{align*}
		m_{ij} = ({\bf x}_i - \bar{{\bf x}})'{\bf S}^{-1}({\bf x}_j - \bar{{\bf x}}), 1 \leq i, j \leq p
	\end{align*}
	and $p$ is the number of variables. For bivariate case, simply put $p = 2$.\vskip 2mm
	$m_{ij}$ is called the squared Mahalanobis distance, and the test statistic for skewness is $\frac{n}{6}\hat{\gamma}_{1, p}$, which follows approximately $\chi^2$ distribution with degree of freedom $p(p+1)(p+2)/6$(so it is $4$ for bivariate case). Also, the test statistic for kurtosis, $\hat{\gamma}_{2, p}$ is approximately normally distributed with mean $p(p + 2)$ and variance $8p(p + 2)/n$.\vskip 2mm
	To test normality, both $p$ values of skewness and kurtosis statistics should be greater than $0.05$ (null hypothesis is normality).\vskip 2mm
	For small samples, the power and type I error could be violated. Therefore, Mardia introduced a correction term into skewness test statistic, usually when $n < 20$, in order to control type I error. The corrected skewness statistic for small samples is $\frac{nk}{6}\hat{\gamma}_{1, p}$, where $k = (p+1)(n+1)(n+3)/(n(n+1)(p+1)-6)$. This statistic is also distributed as $\chi^2$ with degrees of freedom $p(p+1)(p+2)/6$.\vskip 2mm
	{\bf The second method for testing bivariate normality}:\vskip 2mm
	Henze-Zirkler's multivariate normality test:\vskip 2mm
	The Henze-Zirkler's test is based on a non-negative functional distance that measures the distance between two distribution functions. If data are distributed as ultivariate normal, the test statistic is approximately log-normally distributed. First, the mean, variance and smoothness parameter are calculated. Then the mean and the variance are log-normalized and the p-value is estimated. The test statistic of Henze-Zirkler's multivariate normality test is given in the following equation:
	\begin{align*}
		\text{HZ} &= \frac{1}{n}\sum_{i = 1}^{n}\sum_{j = 1}^{n}e^{-\frac{\beta^2}{2}D_{ij}} - 2 (1 + \beta^2)^{-\frac{p}{2}}\sum_{i = 1}^{n}e^{-\frac{\beta^2}{2(1 + \beta^2)}D_i} + n(1 + 2\beta^2)^{-\frac{p}{2}}
	\end{align*}
	where 
	\begin{align*}
		p&: \text{ number of variables}\\
		\beta &= \frac{1}{\sqrt{2}}\Big(\frac{n(2p+1)}{4}\Big)^{\frac{1}{p+4}}\\
		D_{ij} &= ({\bf x}_i - \bar{x}_j)'{\bf S}^{-1}({\bf x}_i - {\bf x}_j)\\
		D_i &= ({\bf x}_i - \bar{{\bf x}})'{\bf S}^{-1}({\bf x}_i - \bar{{\bf x}}) = m_{ii}
	\end{align*}
	From the above equation, $D_i$ gives the squared Mahalanobis distance of $i^{\text{th}}$ observation to the centroid and $D_{ij}$ gives the Mahalanobis distance between $i^{\text{th}}$ and $j^{\text{th}}$ observations. If data are multivariate normal, the test statistic $(HZ)$ is approximately log-normally distributed with mean $\mu$ and variance $\sigma^2$ as given below:
	\begin{align*}
		\mu &= 1 - \frac{a^{-\frac{p}{2}}\Big(1 + p\beta^{\frac{2}{a}} + p(p+2)\beta^4\Big)}{2a^2}\\
		\sigma^2 &= 2(1 + 4\beta^2)^{-\frac{p}{2}} + \frac{2a^{-p}(1 + 2p\beta^4)}{a^2} + \frac{3p(p + 2)\beta^8}{4a^4} - 4w_{\beta}^{-\frac{p}{2}}\Big(1 + \frac{3p\beta^4}{2w_{\beta}}+ \frac{p(p + 2)\beta^8}{2w_{\beta}^2}\Big)
	\end{align*}
	where $a = 1 + 2\beta^2$ and $w_{\beta} = (1 + \beta^2)(1+ 3\beta^2)$. Hence the log-normalized mean and variance of the HZ statistic can be defined as follows:
	\begin{align*}
		\log(\mu) &= \log\Big(\sqrt{\frac{\mu^4}{\sigma^2 + \mu^2}}\Big)
		\hskip 1cm \text{ and } \log(\sigma^2) = \log\Big(\frac{\sigma^2 + \mu^2}{\sigma^2}\Big)
	\end{align*}
	By using the log-normal distribution parameters, $\mu$ and $\sigma$, we can test the significance of multivariate (or bivariate when $p = 2$)normality. The Wald test statistic for multivariate normlality is given as following:
	\begin{align*}
		z = \frac{\log(\text{HZ}) - \log(\mu)}{\log(\sigma)}
	\end{align*}
\end{sol}

Question $4.$
\begin{sol}
	Part $[A]$\vskip 2mm
	part $(i)$\vskip 2mm
	We read in the data and make the scatter plot from the plots option of proc corr:
	\begin{center}
		\includegraphics[width = 14cm]{q401.jpg}
	\end{center}
	The plot output is:
	\begin{center}
		\includegraphics[width = 12cm]{q402.jpg}
	\end{center}
	From the plot there seem to be association between $Y$ and the rest of $X$ variables.\vskip 2mm
	For $(ii)$\vskip 2mm
	We are going to estimate $b$ by the following formula using proc IML:
	\begin{align*}
		\hat{{\bf b}} = \left[\begin{array}{c} \hat{\beta}_0\\ \hat{\beta}_1\\ \hat{\beta}_2\\ \hat{\beta}_3\\ \hat{\beta}_4\end{array}\right] = ({\bf X}'{\bf X})^{-1}{\bf X}'{\bf y}
	\end{align*}
	We build $X$ and $Y$ matrix and compute $\hat{\beta}$ in proc IML with the following code:
	\begin{center}
		\includegraphics[width = 12cm]{q403.jpg}
	\end{center}
	and the output is:
	\begin{center}
		\includegraphics[width = 7cm]{q404.jpg}
	\end{center}
	For $(iii)$:\vskip 2mm
	In the model, $\beta_0$ as the intercept does not give much meaning. If there is no patient data, then there should be no length of hospital stay. In fact our estimate above for $\beta_0$ is very close to $0$.\vskip 2mm
	$\beta_1$ means given fixed $X_2, X_3, X_4$, on average how long the hospital stay will change when the age increase by $1$.\vskip 2mm
	$\beta_2$ means given fixed $X_1, X_3, X_4$, on average how long the hospital stay will change when the infection risk increase by $1$.\vskip 2mm
	$\beta_3$ means given fixed $X_1, X_2, X_4$, on average how long the hospital stay will change when the available faciilites and services increase by $1$.\vskip 2mm
	$\beta_4$ means given fixed $X_1, X_2, X_3$, on average how long the hospital stay will change when the routine chest X-ray ratio increase by $1$.\vskip 2mm
	For $(iv)$:\vskip 2mm
	We compute the eigenvalues and eigenvecotrs of $({\bf X}'{\bf X})^{-1}$ in proc IML as following:
	\begin{center}
		\includegraphics[width = 9cm]{q405.jpg}
	\end{center}
	The output is the following:  (the column $e$ is the $5$ eigen values, and each column of $u$ represents one eigenvector.)
	\begin{center}
		\includegraphics[width = 10cm]{q406.jpg}
	\end{center}
	Notice that all eigen values are positive, and hence $({\bf X}'{\bf X})^{-1}$ is positive definite.\vskip 2mm
	So the data is of full rank.\vskip 2mm
	Remark: I suspected first that it may not be of full rank since one of the eigen value is on the scale of $10^{-7}$ and may be actually $0$ due to rounding error. So I went back and checked the eigen values on $({\bf X}'{\bf X})$ and it came back with all $5$ eigen values positive.\vskip 2mm
	For $(v)$:\vskip 2mm
	Since $({\bf X}'{\bf X})$ has the same rank as ${\bf X}$, and it is easier to handle the rank problem for a square matrix, so we will compute the rank of ${\bf X}'{\bf X}$ instead.\vskip 2mm
	We comopute the rank of ${\bf X}'{\bf X}$ in proc IML as follows:\vskip 2mm
	\begin{center}
		\includegraphics[width = 9cm]{q407.jpg}
	\end{center}
	The output is then:
	\begin{center}
		\includegraphics[width = 10cm]{q408.jpg}
	\end{center}
	The $5 \times 5$ matrix on the left is our ${\bf X}'{\bf X}$ which has the same rank as ${\bf X}$, and the right hand side shows the rank is $5$, which is consistent with our results from last question that all $5$ eigenvalues of $({\bf X}'{\bf X})^{-1}$ are positive.\vskip 2mm
	For $(vi)$:\vskip 2mm
	With the estimation ${\bf b}$ already computed before, it is easy to compute the estimated mean $\hat{{\bf Y}}$ and the residuals under proc iml: \vskip 2mm
	We have the following code:
	\begin{center}
		\includegraphics[width = 8cm]{q409.jpg}
	\end{center}
	The result of $\hat{{\bf Y}}$ and ${\bf e}$ are both column vectors of length $113$, so we only show a portion of the output here:
	\begin{center}
		\includegraphics[width = 8cm]{q410.jpg}
	\end{center}
	The following code gives a plot of residual against the estimated means:
	\begin{center}
		\includegraphics[width = 8cm]{q411.jpg}
	\end{center}
	The output is:
	\begin{center}
		\includegraphics[width = 10cm]{q412.jpg}
	\end{center}
	There is some outliers but most are scattered around $0$ pretty closely. This indicates that the data fits well to the assumption of equal variance.\vskip 2mm
	For $(vii)$:\vskip 2mm
	To create the normal qq-plot for residuals, we have the following code:
	\begin{center}
		\includegraphics[width = 8cm]{q413.jpg}
	\end{center}
	\begin{center}
		\includegraphics[width = 8cm]{q414.jpg}
	\end{center}
	and the output is:
	\begin{center}
		\includegraphics[width = 12cm]{q415.jpg}
	\end{center}
	The qq plot is showing a straight line trend for the residual against the normal quantile, so we conclude that the normal assumption on the data is valid.\vskip 2mm
	For part $[B]$:\vskip 2mm
	For $(i)$\vskip 2mm
	we only need to do minor adjustification to our previous SAS code. I added ``$\_a$'', ``$\_b$'' to some of the variable names to differ between those for part $[A]$ and part $[B]$.
	To estiamte the parameter under the new model without intercept, we have the following code (see my comment in between codes which is for part A which is for part B):
	\begin{center}
		\includegraphics[width = 8cm]{q416.jpg}
	\end{center}
	the output for the estimate parameter is
	\begin{center}
		\includegraphics[width = 8cm]{q417.jpg}
	\end{center}
	This model is different from model 1 because we did not assume intercept in the first place, which should actually make more sense as I explained in part $[A]$, that given no patient information, there should be no length of hospital stay.\vskip 2mm
	For part $(ii)$:\vskip 2mm
	Similar to the process as in part [A], the rank of ${\bf X}$ is the same as the rank of ${\bf X}'{\bf X}$, and we compute it with the following code:
	\begin{center}
		\includegraphics[width = 8cm]{q418.jpg}
	\end{center}
	and the output is:
	\begin{center}
		\includegraphics[width = 10cm]{q419.jpg}
	\end{center}
	as we can see the rank of design matrix ${\bf X}$ is $4$ and hence it is full rank.\vskip 2mm
	For part $(iii)$:\vskip 2mm
	The code for computing residual and plotting scatter plot of residual against estimated mean:
	\begin{center}
		\includegraphics[width = 12cm]{q420.jpg}
	\end{center}
	The scatter plot:
	\begin{center}
		\includegraphics[width = 12cm]{q421.jpg}
	\end{center}
	The residuals are pretty evenly scattered around $0$ except a few outliers, and most stay within rage between $-2$ and $2$, so we consider the independence and equal variance assumption holds.\vskip 2mm
	The code for normal quantile plot:\vskip 2mm
	\begin{center}
		\includegraphics[width = 10cm]{q422.jpg}
	\end{center}
	\begin{center}
		\includegraphics[width = 10cm]{q423.jpg}
	\end{center}
	Output:
	\begin{center}
		\includegraphics[width = 12cm]{q424.jpg}
	\end{center}
	As we can see that the q-q plot display a straight line trend, and hence we consider the normality assumption holds.\vskip 2mm
	The plots we have here are actually pretty similar compared to part [A]. I printed out the residuals and actually find that in both model 1 and and model 2 the residuals are pretty similar too.\vskip 2mm
	part $(iv)$:\vskip 2mm
	The definition of estimability says that, for any given linear function of ${\bf \beta}$, say ${\bf C}{\bf \beta}$, it is said to be estimatble if there is a linear function of response ${\bf y}$, denoted as ${\bf A}{\bf y}$, such that ${\bf A}{\bf y}$ is an unbiased estimator of ${\bf C}{\bf \beta}$.\vskip 2mm
	In our case, we have ${\bf C} = {\bf X}$ and ${\bf A} = {\bf I}$, and since $E[{\bf A}{\bf y}] = E[{\bf y}] = {\bf X}{\bf \beta}$, hence ${\bf y}$ is an unbiased estimator of ${\bf X}{\bf \beta}$ and hence ${\bf X}{\bf \beta}$ is estimable.\vskip 2mm
	Of course the above conclusion holds only when the model is appropriate, which we have verified by checking those model assumptions.
\end{sol}

Question $\#5$.
\begin{sol}
	For part $[A]$:\vskip 2mm
	For part $(i)$:\vskip 2mm
	We have:
	\begin{align*}
		&\ ({\bf I} - {\bf X}{\bf X}^{(-)})({\bf I} - {\bf X}{\bf X}^{(-)})\\
		&= {\bf I} -  2{\bf X}{\bf X}^{(-)} +  {\bf X}{\bf X}^{(-)} {\bf X}{\bf X}^{(-)}\\
		&= {\bf I} -  2{\bf X}{\bf X}^{(-)}  + {\bf X}{\bf X}^{(-)}\\
		&= {\bf I} -  {\bf X}{\bf X}^{(-)} 
	\end{align*}
	So ${\bf I} - {\bf X}{\bf X}^{(-)}$ is idempotent. \vskip 2mm
	On the other hand, since rank of ${\bf X}$ is $k$, so rank of ${\bf X}{\bf X}^{(-)}$ is also k. Since it is symmetric, there exists orthogonal matrix {\bf C} such that:
	\begin{align*}
		{\bf X}{\bf X}^{(-)} = {\bf C}{\bf D}{\bf C}^T
	\end{align*}
	where ${\bf D}$ is a diagonal matrix whose first $k$ diagonal entries are the $k$ eigenvalues of ${\bf D}$ and the rest $n - k$diagonal entries are $0$.\vskip 2mm
	So 
	\begin{align*}
		&\ {\bf I} - {\bf X}{\bf X}^{(-)} = {\bf C}{\bf C}^T - {\bf C}{\bf D}{\bf C}^T\\
		&= {\bf C}\Big({\bf I} - {\bf D}\Big){\bf C}^T
	\end{align*}
	since the rank of ${\bf I} - {\bf D}$ is $n - k$, so the rank of ${\bf I} - {\bf X}{\bf X}^{(-)}$ is also $n - k$.\vskip 2mm
	We already showed that  ${\bf I} - {\bf D}$ is idempotent, and given ${\bf Y}$ that is $N({\bf 0}, {\bf I})$, we know that 
	\begin{align*}
		Q_1 &= {\bf Y}^T({\bf I} - {\bf X}{\bf X}^{(-)}){\bf Y} \sim \chi^2_{n - k}
	\end{align*}
	from Corollary 1 for theorem $5.5$ on page $118$.\vskip 2mm
	For $(ii)$:\vskip 2mm
	Notice that we have:
	\begin{align*}
		{\bf X}{\bf X}^{(-)}{\bf X} &= {\bf X}{\bf X}^{(-)}[{\bf X}_1, {\bf X}_2]\\
		&= [{\bf X}{\bf X}^{(-)}{\bf X}_1, {\bf X}{\bf X}^{(-)}{\bf X}_2]\\
		&= {\bf X}\\
		&= [{\bf X}_1, {\bf X}_2]
	\end{align*}
	This implies that 
	\begin{align*}
		{\bf X}{\bf X}^{(-)}{\bf X}_2 = {\bf X}_2
	\end{align*}
	and hence further
	\begin{align*}
		{\bf X}{\bf X}^{(-)}{\bf X}_2{\bf X}_2^{(-)} = {\bf X}_2{\bf X}_2^{(-)}
	\end{align*}
	we are going to use this in the proof for this question and in the later question too.\vskip 2mm
	Now we have $\Big({\bf X}{\bf X}^{(-)} - {\bf X}_2{\bf X}_2^{(-)}\Big)$ as a symmetric matrix and satisfy the following:
	\begin{align*}
		&\ \Big({\bf X}{\bf X}^{(-)} - {\bf X}_2{\bf X}_2^{(-)}\Big)\Big({\bf X}{\bf X}^{(-)} - {\bf X}_2{\bf X}_2^{(-)}\Big)\\
		&= {\bf X}{\bf X}^{(-)}{\bf X}{\bf X}^{(-)} - {\bf X}{\bf X}^{(-)}{\bf X_2}{\bf X}_2^{(-)} - {\bf X}_2{\bf X}_2^{(-)}{\bf X}{\bf X}^{(-)} + {\bf X}_2{\bf X}_2^{(-)}{\bf X}_2{\bf X}_2^{(-)}\\
		&= {\bf X}{\bf X}^{(-)} - {\bf X}_2{\bf X}_2^{(-)} - \underbrace{{\bf X}{\bf X}^{(-)}{\bf X}_2{\bf X}_2^{(-)}}_{\text{by symmetry}} + {\bf X}_2{\bf X}_2^{(-)}\\
		&= {\bf X}{\bf X}^{(-)} - {\bf X}_2{\bf X}_2^{(-)} - {\bf X}_2{\bf X}_2^{(-)} + {\bf X}_2{\bf X}_2^{(-)}\\
		&=  {\bf X}{\bf X}^{(-)} - {\bf X}_2{\bf X}_2^{(-)}
	\end{align*}
	which says that ${\bf X}{\bf X}^{(-)} - {\bf X}_2{\bf X}_2^{(-)}$ is idempotent.\vskip 2mm
	Finally due to the idempotency, we have
	\begin{align*}
		&\ \text{rank}\Big({\bf X}{\bf X}^{(-)} - {\bf X}_2{\bf X}_2^{(-)}\Big) = \text{tr}\Big({\bf X}{\bf X}^{(-)} - {\bf X}_2{\bf X}_2^{(-)}\Big)\\
		&= \text{tr}\Big({\bf X}{\bf X}^{(-)}\Big) - \text{tr}\Big({\bf X}_2{\bf X}_2^{(-)}\Big)\\
		&=  \text{rank}\Big({\bf X}{\bf X}^{(-)}\Big)  - \text{rank}\Big({\bf X}_2{\bf X}_2^{(-)}\Big)\\
		&= \text{rank}({\bf X}) - \text{rank}({\bf X}_2)\\
		&= k - m
	\end{align*}
	So by Corollary 1 for theorem $5.5$ on page $118$,
	\begin{align*}
		Q_2 &= {\bf Y}^T\Big({\bf X}{\bf X}^{(-)} - {\bf X}_2{\bf X}_2^{(-)}\Big){\bf Y} \sim \chi^2_{k - m} 
	\end{align*}
	\vskip 2mm
	For part $(iii)$:\vskip 2mm
	It is easy to check that
	\begin{align*}
		&\ \Big({\bf X}_2{\bf X}_2^{(-)}\Big)\Big({\bf X}_2{\bf X}_2^{(-)}\Big)\\
		&= \Big({\bf X}_2{\bf X}_2^{(-)}{\bf X}_2\Big){\bf X}_2^{(-)}\\
		&= {\bf X}_2{\bf X}_2^{(-)}
	\end{align*}
	Hence  ${\bf X}_2{\bf X}_2^{(-)}$ is idempotent with rank $m$, so by Corollary 1 for theorem $5.5$ on page $118$, we have:
	\begin{align*}
	Q_3 &= {\bf Y}^T\Big({\bf X}_2{\bf X}_2^{(-)}\Big){\bf Y} \sim \chi^2_{m} 
	\end{align*}
	For part $(iv)$:\vskip 2mm
	$Q_1, Q_2, Q_3$ are all quadratic forms. To prove pairwise independence, by theorem 5.6b and its corollary $1$ on page 120, we only need to show that the multiplication of their quadratic matrix is ${\bf 0}$.\vskip 2mm
	We have:
	\begin{align*}
		&\ \Big({\bf I} - {\bf X}{\bf X}^{(-)}\Big)\Big({\bf X}{\bf X}^{(-)} - {\bf X}_2{\bf X}_2^{(-)}\Big)\\
		&= {\bf X}{\bf X}^{(-)} - {\bf X}_2{\bf X}_2^{(-)} - {\bf X}{\bf X}^{(-)}{\bf X}{\bf X}^{(-)} + {\bf X}{\bf X}^{(-)}{\bf X}_2{\bf X}_2^{(-)}\\
		&= {\bf X}{\bf X}^{(-)} - {\bf X}_2{\bf X}_2^{(-)} - {\bf X}{\bf X}^{(-)} + \underbrace{{\bf X}_2{\bf X}_2^{(-)}}_{\text{proved in part (ii)}}\\
		&= {\bf 0}
	\end{align*}
	Hence $Q_1$ and $Q_2$ are independent.\vskip 2mm
	We have:
	\begin{align*}
		&\ \Big({\bf I} - {\bf X}{\bf X}^{(-)}\Big){\bf X}_2{\bf X}_2^{(-)}\\
		&= {\bf X}_2{\bf X}_2^{(-)} - {\bf X}{\bf X}^{(-)}{\bf X}_2{\bf X}_2^{(-)}\\
		&= {\bf X}_2{\bf X}_2^{(-)} - {\bf X}_2{\bf X}_2^{(-)}\\
		&= {\bf 0}
	\end{align*} 
	Hence $Q_1$ and $Q_3$ are independent.\vskip 2mm
	Finally we have:
	\begin{align*}
		&\ \Big({\bf X}{\bf X}^{(-)} - {\bf X}_2{\bf X}_2^{(-)}\Big){\bf X}_2{\bf X}_2^{(-)}\\
		&= {\bf X}{\bf X}^{(-)}{\bf X}_2{\bf X}_2^{(-)} - {\bf X}_2{\bf X}_2^{(-)}{\bf X}_2{\bf X}_2^{(-)}\\
		&= {\bf X}_2{\bf X}_2^{(-)} - {\bf X}_2{\bf X}_2^{(-)}\\
		&= {\bf 0}
	\end{align*}
	Hence $Q_2$ and $Q_3$ are independent.\vskip 2mm
	Thus completed the proof.\vskip 2mm
	For part $[B]$:\vskip 2mm
	Question $5.20$ from the book:\vskip 2mm
	Theorem $5.5$ says that, if ${\bf y}$ is $N_p({\bf \mu}, \Sigma)$, and ${\bf A}$ is symmetric with rank $r$, let $\lambda = \frac{1}{2}{\bf \mu}'{\bf A}{\bf \mu}$. Then ${\bf y}'{\bf A}{\bf y}$ is $\chi^2(r, \lambda)$, if and only if ${\bf A}{\bf \Sigma}$ is idempotent. We use the reuslt of this theorem to prove the other corollaries.\vskip 2mm
	For part $(a)$:\vskip 2mm
	Given ${\bf y}$ is $N_p({\bf 0}, {\bf \Sigma})$, we have ${\bf \mu} = 0$, and hence:
	\begin{align*}
		\lambda = \frac{1}{2}{\bf \mu}'{\bf A}{\bf \mu} = 0
	\end{align*}
	for any symmetric matrix ${\bf A}$.\vskip 2mm
	Hence by Theorem $5.5$, ${\bf y}'{\bf A}{\bf y}$ is $\chi^2(r,0)$ if and only if ${\bf A}{\bf \Sigma}$ is idempotent of rank $r$. But $\chi^2(r, 0)$ is just $\chi^2(r)$. Proof completed.\vskip 2mm
	For part $(b)$:\vskip 2mm
	Given ${\bf y}$ as $N_p({\bf \mu}, \sigma^2{\bf I})$, we have $\Sigma = \sigma^2{\bf I}$, and if we consider ${\bf y}'{\bf y}/\sigma^2$, we have ${\bf A} = \frac{1}{\sigma^2}{\bf I}$.\vskip 2mm
	Thus 
	\begin{align*}
		{\bf A}{\bf \Sigma} &= \frac{1}{\sigma^2}{\bf I}\sigma^2{\bf I} = {\bf I} \text{ which is idempotent}
	\end{align*}
	Hence ${\bf y}'{\bf y}/\sigma^2$ is $\chi^2(p, {\bf \mu}'{\bf \mu}/2\sigma^2)$\vskip 2mm
	For part $(c)$:\vskip 2mm
	Given ${\bf y}$ is $N_p({\bf \mu}, {\bf I})$, we have $\Sigma = {\bf I}$, hence the condition of ${\bf A}{\bf \Sigma}$ is idempotent becomes ${\bf A}{\bf \Sigma} = {\bf A}{\bf I} = {\bf A}$ is idempotent with rank $r$. Proof copmleted.\vskip 2mm
	For part $(d)$:\vskip 2mm
	Given ${\bf y}$ is $N_p({\bf \mu}, \sigma^2\Sigma)$, we have $\frac{{\bf y}}{\sigma}$ is $N_p(\frac{{\bf \mu}}{\sigma}, \Sigma)$. We can denote ${\bf y}/\sigma$ by ${\bf z}$, then when we consider ${\bf y}'{\bf A}{\bf y}/\sigma^2$, it is equivalent to thinking about ${\bf z}'{\bf A}{\bf z}$. Since the variance matrix for ${\bf z}$ is $\Sigma$, and the mean is $\frac{{\bf \mu}}{\sigma}$, so the $\chi^2$ distribution's noncentral parameter is 
	\begin{align*}
		\frac{1}{2}\times \frac{{\bf \mu}'}{\sigma}{\bf A}\frac{{\bf \mu}}{\sigma} = \frac{{\bf \mu}'{\bf A}{\bf \mu}}{2\sigma^2}
	\end{align*}
	and the necessary and sufficient condition is for ${\bf A}{\bf \Sigma}$ to be idempotent.\vskip 2mm
	For part $(e)$:\vskip 2mm
	We can use the result from part $(d)$, take ${\bf A} = \Sigma^{-1}$, then 
	\begin{align*}
		{\bf A}{\Sigma} &= \Sigma^{-1}\Sigma = {\bf I} \text{ which is idempotent with rank } p
	\end{align*}
	Hence from part $(d)$, 
	\begin{align*}
		{\bf y}'{\bf A}{\bf y}/\sigma^2 = {\bf y}'\Sigma^{-1}{\bf y}/\sigma^2 \sim \chi^2(p, {\bf \mu}'\Sigma^{-1}{\bf \mu}/2\sigma^2)
	\end{align*}	
	Thus completed the proof.
\end{sol}

Question $\#6$.
\begin{sol}
	For part $(i)$:\vskip 2mm
	I confirm that I have read and understood the material on Singular Value Decomposition and know that it is a generalization of the case of eigen values and eigen vectors. (I have also learned this from the numerical analysis class I took from my math degree)\vskip 2mm
	For part $(ii)$:\vskip 2mm
	I confirm that I have read and understood the material on the relationship between determintants of $2 \times 2$ matrix and area of parallelogram. \vskip 2mm
	Some of the proofs are more intuitve than the others but actually my favorite proof is the following (not given by the link), which I taught to undergraduate students at KU math when I used to work there as a lecturer.\vskip 2mm
	Given $(a, b)$ and $(c, d)$ as $2d$ row vectors, consider they are special case of $3d$ vectors: ${\bf u} = (0, a, b)$ and ${\bf v} = (0, c, d)$. By definition of cross product, we know that $|\text{det}\Big({\bf u} \times {\bf v}\Big)| = |{\bf u}||{\bf v}|\sin(\theta)$ is the area of parallelagram formed by ${\bf u}$ and ${\bf v}$ . (Here $\theta$ is the angle between ${\bf u}$ and ${\bf v}$). We also have:
	\begin{align*}
		{\bf u}\times {\bf v} = \left|\begin{array}{ccc} {\bf i}&{\bf j}&{\bf k}\\0 &a&b \\0 &c&d\end{array}\right| = \Big|\begin{array}{cc}a& b\\ c& d\end{array}\Big|{\bf i}
	\end{align*}
	So 
	\begin{align*}
		|\text{det}\Big({\bf u}\times {\bf v} \Big) |=\Big|\det\Big(\begin{array}{cc}a& b\\ c& d\end{array}\Big)\Big| =  |{\bf u}||{\bf v}|\sin(\theta) = \text{ area of parallelagram formed by }{\bf u} \text{ and } {\bf v}
	\end{align*}
\end{sol}























\end{document}
